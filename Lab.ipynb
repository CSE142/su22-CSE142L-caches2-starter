{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from notebook import *\n",
    "# if get something about NUMEXPR_MAX_THREADS being set incorrectly, don't worry.  It's not a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": false,
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"namebox\">    \n",
    "Double Click to edit and enter your\n",
    "\n",
    "1.  Name\n",
    "2.  Student ID\n",
    "3.  @ucsd.edu email address\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div style=\" font-size: 300% !important;\n",
    "    margin-top: 1.5em;\n",
    "    margin-bottom: 1.5em;\n",
    "    font-weight: bold;\n",
    "    line-height: 1.0;\n",
    "    text-align:center;\">Lab 4: The Memory Hierarchy (Part II)</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "This lab is a continuation of the previous lab.  While that lab focused on the basics of cache-aware programming and spatial locality, this lab will focus more on temporal locality and how you can modify your programs to maximize it.\n",
    "\n",
    "As a reminder, between these two labs, you'll learn about the concepts of:\n",
    "\n",
    "1.  Memory alignment\n",
    "2.  Thinking in cache lines\n",
    "3.  Working sets\n",
    "4.  The cache hierarchy\n",
    "5.  The impact of miss rate on performance\n",
    "6.  The role of the TLB in determining performance \n",
    "7.  Spatial locality\n",
    "8.  Temporal locality\n",
    "9.  Cache-aware optimizations\n",
    "10.  The impact of data structures on memory behavior\n",
    "\n",
    "Along the way, we'll address several of the \"interesting questions\" we identified in the first lab, including:\n",
    "\n",
    "* Why does increasing the size of array change `CPI`? And why does this change occur so quickly?\n",
    "* How and why do the datatypes we use change `IC` and  `CPI`?\n",
    "* Why does the order in which the program performs calculations affect `CPI`?\n",
    "\n",
    "This lab includes a programming assignment. \n",
    "\n",
    "Check the course schedule for due date(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# FAQ and Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "* There are no updates, yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Additional Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "If you want to learn _a lot_ more about optimizing matrix multiply, try this paper:  https://www.cs.utexas.edu/~flame/pubs/GotoTOMS_revision.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Before You Do Anything Else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run this cell.  It'll fix your git repo history so you can successfully merge in updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./fix-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Pre-Lab Reading Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Part of this lab is a pre-lab quiz. The pre-lab quiz **has moved to Canvas** so I can allow multiple attempts.  It is due **before class on the day the lab is assigned**.  It's not hard, but it does require you to read over the lab before class.  If you are having trouble accessing it, make sure you are **logged into Canvas**.\n",
    "\n",
    "##  How To Read the Lab For the Reading Quiz\n",
    "\n",
    "The goal of reading the lab before starting on it is to make sure you have a preview of:\n",
    "\n",
    "1.  What's involved in the lab.\n",
    "2.  The key concepts of the lab.\n",
    "3.  What you can expect from lab.\n",
    "4.  Any questions you might have.\n",
    "\n",
    "These are the things we will ask about on the quiz.  You _do not_ need to study the lab in depth.  You _do not_ need to run the cells.\n",
    "\n",
    "You should read these parts carefully:\n",
    "\n",
    "* Paragraphs at the top of section/subsections\n",
    "* The description of the programming assignment\n",
    "* Any other large blocks of text\n",
    "* The \"About Labs in This Class\" section (Lab 1 only)\n",
    "\n",
    "You should skim these parts:\n",
    "\n",
    "* The questions.\n",
    "\n",
    "You can skip these parts:\n",
    "\n",
    "* The \"About Labs in This Class\" section (Labs other than Lab 1)\n",
    "* Commentary on the output of code cells (which is most of the lab)\n",
    "* Parts of the lab that refer to things you can't see (like cell output)\n",
    "* Solution to completeness questions.\n",
    "\n",
    "\n",
    "## Taking the Quiz\n",
    "\n",
    "You can find it here: https://canvas.ucsd.edu/courses/29567/quizzes\n",
    "\n",
    "The quiz is \"open lab\" -- you can search, re-read, etc. the lab.\n",
    "\n",
    "You can take the quiz 3 times.  Highest score counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Browser Compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We are still working out some bugs in some browsers.  Here's the current status:\n",
    "\n",
    "1.  Chrome -- well tested.  Preferred option. **Required for Moneta**\n",
    "2.  Firefox -- seems ok, but not thoroughly tested.\n",
    "3.  Edge -- seems ok, but not thoroughly tested.\n",
    "4.  Safari -- not supported at the moment.\n",
    "5.  Internet Explorer -- not supported at the moment.\n",
    "\n",
    "At the moment, the authentication step must be done in Chrome.  You usually _will not_ have to re-authenticate between labs, so if things work OK for the first, things will probably work here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# About Labs In This Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "_This section is the same in all the labs.  It's repeated here for your reference._\n",
    "\n",
    "Labs are a way to **learn by doing**.  This means you _must_ **do**.  I have built these labs as Jupyter notebooks so that the \"doing\" is as easy and seamless as possible.\n",
    "\n",
    "In this lab, what you'll do is answer questions about how a program will run and then compare what really happened to your predictions.  Engaging with this process is how you'll learn.  The questions that the lab asks are there for several purposes:\n",
    "\n",
    "1.  To draw your attention to specific aspects of an experiment or of some results.\n",
    "2.  To push you to engage with the material more deeply by thinking about it.\n",
    "3.  To make you commit to a prediction so you can wonder why your prediction was wrong or be proud that you got it right.\n",
    "4.  To provide some practice with skills/concepts you're learning in this course.\n",
    "5.  To test your knowledge about what you've learned.\n",
    "\n",
    "The questions are graded in one of three ways:\n",
    "\n",
    "1. \"Correctness\" questions require you to answer the question and get the correct answer to get full credit.\n",
    "2. \"Completeness\" questions require you to answer the question.\n",
    "3. \"Optional\" questions are...optional.  They are there if you want to go further with the material.\n",
    "\n",
    "Some of the \"Completeness\" problems include a solution that will be hidden until you click \"Show Solution\".  To get the most from them, try them on your own first.\n",
    "\n",
    "Many of the \"Completeness\" questions ask you to make predictions about the outcome of an experiment and write down those predictions.  To maximize your learning, think carefully about your prediction and commit to it.  **You will never be penalized for making an incorrect prediction.**\n",
    "\n",
    "You are free to discuss \"Completeness\" and \"Optional\" questions with your classmates.  You must complete \"Correctness\" questions on your own.\n",
    "\n",
    "If you have questions about any kind of question, please ask during office hours or during class."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIkAAACOCAYAAAAFIStxAAAgAElEQVR4Ae2dh5tV1dn2v7/ild6rooiAoBhjV+wNS+waYskbY4yvKSbGHrHEGBUrRukBBIWBAAqCSEe6ooJ0FJA6M0w/5/6u37PWc2bPYYYyM4rDzFzXuvY5e845e++17nU/dT3r/6nhr6EHDtID/+8g/2/4d0MPqAEkDSA4aA80gOSgXdTwgQaQNGDgoD3QAJKDdlHDBxpA0oCBg/ZAA0gO2kUNH2gASbpUKlgvbf9YWj9I+upxacX/SSsekFY9Km0cJu1eJJXtrbdoqccgSUu5n0tfPSfNv1n6+Bzpwz7SR6dJ006Xpp8e3k8+Rfq4r7T4t9Lm96SSPfUOLPUUJClpy3ilP7pImtJHmnGGNOc8af6F0vyLpAUXh+O8vtLsc6WPfy5N7i1N7CPN/5W0e7GkdL0BS/0EyeYcFX/QR5rYS5p5pgQYFl4mLb5aWnKNtORaaUk/afFV4Tz//+RsadrPpJyeKs35ubR+sFSWXy+AUv9AkrdOxZMvUtl7JwbRAlMsvDSAY+mN0rKbY7tJ4j2gAUAA5dNzghjK6amS97pLnw+QynKPeqDUO5Ck1gzRviFdpJwegRlmnyctukJaeoO07FZp2e2JdlsACgxjQLlA+uQsA0r6g+4qGN5VqaWPSql9RzVQ6hdIUoUqm/trFQ47Tpp4cgDJnPODWFl6UwTHL6Vl3u4IwAFABpRLg+6CiPqwj7FR3rvHKbXyOUmlRy1Q6hdIiveqaOrlASTOJCisAMBYxMHBsX8EC0C5RVp6XQATiu3sc6QZP5emnKLS0V2VN6SrytaNaQDJUdEDJXkqnHq18t/tLMSFmbsZkNxWziDL+0veDCy3SzANCi2iad4F0qdnB6tnUi8VDD1OeaNPV3r350dFN2U/RP1iknSJSuY/qNy3Oqps7InBD4Lims0kDhA/mvhJ6CcLotj55EwDWmpcN+15raOK5j4opYqy+7jOv69fIEFzWDPMBrRk5PEmLvTpudKiq4JIMTA4i/xKWu4N0XOHtPSWcjaZH60dfCiRTXYOOkHataTOgyL7AeodSFLbFyh3WA8TEeYnwVox6wbTF/2jMpD8KuoosAlK7JXSgosSSuypgk12vdZRhZ/el93Hdf59vQOJirapcOpVyn+nszShR/C24icxHwkg+WXUR5xF/OhscnO5pYNuMuusjJON39w9tE+IBdV5aJQ/QP0DiaTC2fcrb1Anpd8/KSifuOGXXF9uApsu4uBIHE2JvU1acl3wmyBy0Glw6085RSUjT9DONzqr7Ot3ynv4KHhVL0FS8sVA5b1zbPC64mrHm8rAL3MLB5GTAEfmNWyCpXNDEFGYw1hH+E0+Os1At+vVDto3415JqaMAHuER6iVItHW69g3vrpJRJwQLZ+4Fwf1eASSVAcVFzo0V9RL0GiLHOT2V93Yn5Y09X8pf1wCSOt0DBRuU/97PVTi8S7BwqjSDK2ETs3IAyVXlyivBv4SVs/vfXVW2dW6d7qLkzddPJkmXKD/n8mDhTO4VAneLroyBvai8mpVTFUhuCsorFs7c88P30Usm91bx8OO1/V/tVbJmXLKf6/Tr+gkSSUVz7tO+IceGGI6bwRYBPhQLB+9rv5B3kgTJlFNUPOJ4bf1HW5V8MahOAyN58/UWJOk1g0x5NTMYxXPh5cH1br4SN4Mr0UtM3DiTXCwRIHRxYxZOAEnx0uePmsSkegsS7Zivve92CTGcGYDkskp8JdkgccU1gsSsG0ByViZ7DU/ut8+1VfGSZ44aC6f+giR/o/KHdzdPqfk5iMckHWqVel4dJNmK65khGWlyLxWP6KLNz7RV0WdPN4AkKbPq5OvinSrKuUClY7qGsH9NQIK4InF64smmDG8e0EaFiwY0gKROAiN502X7VPbpHeYlNfM1GyQeAc440rB03Jl2o4Q1hBPOckuCMw39hjSEjU+1UcHSV5JXq9Ov66+4SZcovfiPKh55QuVMsh9IAAgtBvkICuKEc31k6qlKvd/N0hAASdGXI+s0MJI3X49BUqbU8icDk5BlVmmQD8XVlVcAQsZaInbjLnlc+5N7m+jaNbC9trzQQcXrpyT7uU6/rtcgSa8cEHWSM/a3bvZTXB0kt4acEoKCZM/jaWVRV05P8+Bu/2c7bXvjJKW2f1angZG8+XoMklKllz2qFJFgFM9Kc0oSLGL6CExyi7S4nzTvwvA9WGRKbwvukSqw5dm22jH8Ail/Q7Kf6/TreguSdNk+pebdHXJKyFc1t/wtUaS4mHGQeNIR3tiYT4I+giseFpnYU6WjT9CeNzpo49/baPekXx9V2fP1FiQq3K6SiWeH5ZusvamQ5+ogScRuTGklhRHL5oqQIoComXqKNL67mb7fv9RO659qq71zX6zTzJF98/UXJFsmqXRs95BVZqkC5JOwMCsup8gorBEoSZDgnbXVfCisvZQae6Ly/93JPK0bnu+q0i2fZPdznX5fP0GSLlPp7P7RJX9GWBxuK/gASSUsgq8kGyTEa6hAMPFky0vZ+2ZHbfp7G21+41yp+OgqU1E/QbJrsUomnhmz5c/OsmwOFSRnSR+eZjoNrngy0tY/0UY7cxoSoes0bfrNly0foLKxJyWWeSZW8GWLmYzHFRM46iT4VHCiobSO72ErAncObK+Nf2+nghWD/TJHzbH+MUneOpV9dJn5NUT0l5okmfzWKljEgHJgkOBE2/hsJ5V8t/CoAYc/SL0DSXrtYJWMoexEn7BUM+lprZJFPG6TYBLMZnSSCT0s0cg8rS91U1nut963R82xfoEkf5OK/3tB8I1gvs7NMn0PGSRYN+UgIYcEnWTr26crVbDzqAGHP0i9AknpkkdU9l63oEvgZa2w3gZRk/CL7Pfaxc0NQdEl+ou3dWJPs272vNZBO4aeo3Thbu/bo+ZYb0CS3vKhisadEnJaYZFMXZLoZT0gi7gJzJqb6wNIvOrRpF7mbcUE3jHkdKULG5ikbs6OvPUq+fBKpcZ2C3oElonlj1Dd6PbyMhP7sUeCWcxPQgQ4goRlGCQaTe5tzjTW2+x4s5vSeRvrZh8d4K6PeiZJp4pUNPd3KhzWxVb/W7yFNbxku9tirANZNEmQVJImQOxm6qkSpbGGHqudr3VW2eZpB+juuvmvoxsk6ZRKP3/RaptZVjw6BAxgEV/KX3lWfAIMVbGJuetvDRWPFl0exBV6DRYOhfZGEeDrqIJPHzhqsuQd0kc1SFIbxmrfqJ4h2ZkZ78oqeoXFaQ6mrCbAUwEkyay0081zy+Jz1vHsHXayUtvmeP8eFcejFiTpbZ8qf8xpIfOMQr1m8l4QckF8zW9VrFHZeVuP40xCSaxYxAaHHGwyKQb63umsvHHnKb1lekMi9E96iuxapvz3zgzLOHN6BlPVxAxLOd2aSbBEZaCocC6avxTfQ3GtsA74zJiddmoAyrhuKhhyrPJG9VLpimeVLtgkpet2hYGjjklSu1Yqb9y5YQnn+Fg8j4jtIXtWKwMPIImKK/kkZKZ5XVdMYdgEfYdYDqw1PlQsyB9ynPZ9cLZSX7wo7V0lpUt+0nOrqps7qkCS3rtG+RMuidUVTwqWB3rIvIvK4zMH84dUYJAIGDN/I0jITCPWQyab1yfxjHmAgu4DUHKCLlQ88njtG3KciiacrbIlD0vbZ0mpgqrG4yd5/qgBCQDJnXCZ8v/dOSiqU04J6YUkFDHzM3rIYSirGcAkmASRA5tQThwrx9benFu+tAL9hGw1qhVM7ClNCKsEsX4Khx+vorG9VTTteqXXvi3lfaN0GYA52GYG/D8lpctCswI5B/tO7eHtqAAJ9VNzx/e17LCy97qGmZxRVK8KVRMPx9zNgMNFj4OEHFcHCcV/YzlxIslWXSDWKTGgnBryVag8Pb67OfLIg6UaNQnTeW93Vu7wniqc8UuVrh6q1HezlN4yTelNE5TeOEpaP1Ra97a05nXp65ekL1+QPn9Gadqq56WvXpTWvKH0miFKbxyv9PY5Uu7XUgkJT7ULoDoPkvTer5U/4eIAEGqzuiXDmhhEgpUDvyOxfsYH/lCO7mirBCRUiIZNAAp+F3QeWCvjOznZ4kSUogAYFAQupA3vYlFjOz+8iwqGdVHhiBNUOKanSsacpLJx3S2Dn2qOtLKx3axsF0d7/96JKhvT1c6xRNXa2JNUNuE0pT66WGVz+qsUHWj38lqzruo2SHLXKj/nEpuZ5JlWBAibCrDrRHUB4iCKQEnqJfwuvhZ0EwMKVY8utpJY7H6xb/Cxphdh5RQOP84cbQwwCUpW0x4go7vAODQUXkQUr2FAgOYNry7nPj4jiE/eZ85x/vRypXlSL1vaUTL6RBW+f4ZKP39BKq75Lhp1FyR561Q4JZQAt5gMOgidSeDOGASAeFzGB/xwjkkWcSZJWDgOEpjk0/OVGtfdgMFaYNIZES0GDMqTo5twfyi2WEIUGJ5zQVi7g/KLYs1xwWWhTgq6DuxkDHW5tBCm8vfxNfVUYC++B4Nh4gMsroHPZlw3FY/qpuK5v1O64LsaKSh1EiTpwq0qnnGbWQ22uIpZyOyis2obIFhDZv4CENdJbogbElwtfXyWiQzECfoQg2OlPzG/0UcABwzBACICGVSKBcNCJF+bOPQ9dvDhJF/znsYWK978HJsi+J48cXME2IxrsGz1w1NNFyoccbyK591fo+TsHxYk1Fkv+l7CoVS4TSreGRQrdpxKFUa/QdnhKVpl+Spd8HsVDO8SBgOqZgDMikFJjQyCeKiOuWtKq3/XRQ0AoZFNf3MUNf2kGWeZaEG/sP1zJp0cgDGpVxAf3BuRYgCMPwVLCHYwgAAGSoLym4CP5kD06x3s6N/jNyhpfn34fZRo+uTDPqYw5w8+XqVrhlSbTX44kOSukZY/rNL/nq3i909R8fjTVTr5PKWms7DpVmnhvdLyP4eNEtHgN4yQNudI2z+Rdn8WNlEsWCcVbpKKtktleeZfSK/6p4pHnmiRV5uh+ChYcgnt26w8wLKI/ayWA4kfGMSbD15iE4I5F6hg2PHlFRzRMQCF6xnQPs11DMSBsQhAJkUBgDDI8RoG6CxwHuicfy/DcuS6xNr3pkRHoEzuLTLn9o09Q+mCzdUCyg8Dkr0rlZpxje0FEzRxtPGoiY8+wW6a2YejiQfItNFdg3af00fpKWcrNe1SpWf0U+qTG1U251cqmdVfRWN6Go3aAJBCaB3fr9yKqRGDOGicQVzUMJgMKoNwneWipCecbD6ZjLIMGKajSHqLO4Lynt0sKgXJYUShKwA8C0wZoMSKB7CVxZbCvoEh+NhFqa/f+omApGirNPfmkEeKTK6qEVPJtB7h89R6H9/dxAgPhr7BvjTpD07K+BdM3kPhdDolMrEulkLdcVZWW8RkAYTfydA/DIIOEIvXzIxihtom3AvU7q2C5fHzoB/wv4y4wSz3rdyqA5JsgCTvM4LYQBL35Jn2M+tPFOp9U6+PzrjDw0rtMgl70a16WJoC9XrrU07DLGZyOk5Ss5+r7AhlTz3VQGKbKzIo6B9o92xSxOBlwv50oA92dY/OIkkREwGy+Bppbl/bBZTcEctRARQOEI5VgYT4EUolosDuO27nlhEp3G8SAP7ej8n/+T0mAZIQhWTdYeVxPx+dZnqJVaoec4bEJD7Mv9oFyZax0oyY1pehXadfZlWWrY85mOxgfw09o2vQmIETe5lIMplvCiqzkZwQNH73g9Bx1QVG8nt0PC2KGGcQnGcE9WaeGRKMXusYMt0qe06ATBlx34Saz/BsGTZBeb1eWpp9/0kgOMv4/SSPrtByj36fhAp8Zy/fGMG3gDtBJGrvGdZb2nv4u3vVHkjyv5EW3hSAQIfYIJ8dOga/AI3V+8ymTDs/uLMZeG+kFqKIomswI+b2DVYMFoMlL6OgYsFExS8z+5IDXd3XPhAMQkLE2P57V4Z7/LCP8v7dyTZWwh9h4mZ6VFJdWeUIK/p7QONA4dkxVYknZZ4j28JxECSPDgiOfJ7np2FteSzpitBvSZ8JxXWGHacdL7XTrsG9pdwvDpNHpNoBCYGnDQMDIOgETDAG2wb64jDYzEJzAEXnEHKTHauwSjKNDZt982b8CNdLs/sGX4MBxPUPOikx02qLQTJ6CL8fYzQGEGI0lwY2mNBDuwZ20I5X2gdx42Aw8RmtG157YxEYnwEoMCnKNn2TMYUBvPtCfOArO8I6fA5QRP+IOfSujaGByyNA4q4ZAHRSLzMYEDXb/tlOO4edIZUcfjZ/7YCkcL20+Ppwk3QmHUBiDrMFCjT3NfSK8wiXNo0HReF05xCdkOycOFBzCZ5lbTVS2+zhv2dixlmE+yPBKMZmsBam/cw6/ft/tdO3z7cN2fc48gAERxxnfvTXvHfzGKDAsjCqm8PmM6FvvF8qO9JvfIYwgIcCuK8rQyQaxgV4MIhnyk0OLnpSKne+0l6bnm6j/Gm/PDyfVOSc2gHJt0MiKGAB9yRWNkMY+Eiby5NMUIU+AWiclit8vrriJPm9hPyv4HNAzDBb2UatXwA8UV70iVg7nlm54YnWIfMN641YDKKHbe052mssu8T7Kb2DMs8MBygwLkDBBW8TCkaFSb05wzLZroqAwCWfcMcDXMQ0oEO8I9IAbE5PAzOxoz2vdrASXeuf7qjUlhmHLWr4Qs1BUrJH+urByBJJL6IDIhsMVQCiUpHBrK5J/CUJCn+dAIeJl6QekhAzGSXwwjhDzzBznuAdGwx89VBL7XipvQ2GeVuj+Y4Jv1/jf+xDDHgyMZzohWWQAYuJ5nj094CTBhhgCsQ44IIxEFuADREGQwGOiTFmM7yL7bsDg2x+po2+/ksr7Zl8V7WTnWoOkj1zpZV3R1GBpeGg8EH5qR0TIKnAIG4lsANFcqPoROBsfHeLOFM8b+UDLbT6r63Eyj0chgQZiUTv12LI32I6BPsADMyDnuLKrFtymMmZ19G6M0svWoEAgu8AChgpkwV3stIE9EYcbykTsAdVIKnf9uWfWmrz62cptXt1tVikdpgEUYNYcLleKSMcCaAkwJC5t6xzGR0kMhb6kYmZqKgya5mtDMyUUyx4R+bb5gFttPx3LbTgzuZa/XArU2T3vdvZFmiRGlBsOSNdyj3JeJVHnaDSMScY85hDEEdidsoAwPGGT8l0nVPL9Rw+jwjjuyQyjetm0eaCYccF5hjY3hRUKi6t+VsrffmnVtr8Zl+VbK3ZNrQ1Y5KSHdI3AyJ7HAkgHOiaWYDIBkqlLOKmZExLRA9h9jJwWApjutpM3fIMTNJSM25qqo9vbKoVD7SwWWsWxMD2xi5sUM1mkFgW1FPLf6eT9g3ubOYoYYhMxBh2Ic9kv4anOXifAQOORK4f0iC7WGCR38b/wXVhjTUPt9IXD7bUst+11Kq/ddH3E+5XWd6WajOIf7FmICn4RvrygVpyYh1owKvzv0MBifshorJqYoY9f2PIHRaB2qH1nFCGM//tTtr6fFt99eeWmtu/mT64srHGXdFYM28LYFn3aGuzJNjOBN1l2wttbRC3/6uddrzcXtQx2ftGxwAeXOVDjrUlGCiZ1oaGLDZeAyoYCoABOEQbJS6wrrgHGGPtI60MsPPvbKYp1zfRmEsbad6fT1fR6pwQafeRrsGxZiDZs0BacedPECSHAhAUVkCCLuJ5qyyV8OTmc4KoQfajbI7vodJRJwiQbH+xnQ3O0t+20PQbmmp430Yacv4xGn1JIxuoBXc317LftbBZjU7w1UOtTCyteaSV1j3WWhuebGNA2vxsW6vYyIDTABS6hFWVBlwvhPMonzAF30UJXflgCy25t7nm9G+mydc10ZhLGmnw+cdo0Ln/o8EXHKPpv+qor1+7Wflf/FcqqXlmfg1AkpK2ja9BzkZ12OFQvnMQgOxn0cAi+EQSiTtYEW5SImrIfB/f3eQ/ScwwwsanWuvLP7bUonuaa8q1TTTiwkYadXEjvX9lY035RVN9cnszzburuebf01zz726uBfc018LY+M7iXzfX4v9triWxLf1Ncy29t7n8uOQ3ze23F9zVTPP6N9OsW5vp4xua6sNrmyjnqsZ2nfevaqwPrmmsSb9ootl3NteKB1rqm8dgsrba8Pe2WvtUR+2e8nuV7a1ZpYPqg4SFRpvf/gmyCEA6CFAyCmuSRVgiEd3arrC6aYnp+kEACSLASl89G5TDFfe3MN1k+o1NtfS+Flr7SGuhs3z/ryBWcl1MvNZROwd21I6XO2jHSxw76fuXOmn7vzpr2z8767sXOuu7f3TWluc6adOATtr4dEetfbKD1jzeXmsea6/Vj4bj2ifaa9OADtr2QkftfAWd51gVDEUxJh2DZOmTgkI7pqsKhhynPW92Uu74q5Te+021BU71QcICo3XP/QRBchCAZJgkWjTmOIueVVuVl1jj62ZmEiQ4qKKyuOGp1lp+XwstuKu5mcOIB/QNdAjiJeS5ks3uOTWpcScGHwoOODyxJDFj4uLzmEPeK/Et2jnhHFvcuwnMkfd8ls8QpkjGu/BKw4CmbMdcV2qnjOum/HePVcH0W6udwlh9kJAp9vVff0BxkxzsQxEzh8AgzjAZJkHUYNEgaqLCmi1q0Ecm9jJLgwE3c3NQJ3N1b3yytYkHzE1EEPoK5q8lZpsDjZyZmCuD2ZpxpgEOMuo8nRFPNdn33tz9TlyG/yXiWdxrhUbYI7G0I6l0IyonnmxAJYvOsuerwSfVB0nxNunz31VC7dkD6oOdfb6q9/757GNVn/fz2Z8/wHsDSUz3wy9CQRsTNdG76laN6SP4Jk42cYMZSiY8TAGbIFpQTAEI1gggMm8rvgyYokJMJy6hQITx+7AA1+T6lvJA3AqPLy0Zw8oO6iVjOzGmY6ABLIk1yoAQJoxBPnSp/LFnhnzjwwRK9UGC+bvy1yH3wmfoYR2zB5f3BxjYzP/8e4f6+azfrMAirrDGlXi4vqFrBwmDjAMrJ67Cc+fV0CBy8EsQOKPKka37cfaAMTIgwRkWG6ADJIgOA0nMUrO8EgdJEiAwHQ6+ZOTXQQJAALgzS4wG+0J2vLeAZGKIBOOr2f3WsSpbM/wwIVKT2E3ucmkF7nj3NSSOmQHNGqBaPX+YIDFwuNmbFaNxUYOcR+6zJMH1kQRIcH3jBMMZRrqAiRmP3+AMs3RMX4BFnKZ3osVoML+Lg46gHANq6ZcMOJlqHhEHGAA4CQiiwL4gDGBEMQN7WFCQIOAVwcdD6VGA7uJmdFfzs+CrKVz05I8Ikt1zpRV3lUd1s8FSq4CoIdgqAMRjNMxOOj2KGvJlsWrc9GUwScFMgoS827EnGkgI7gGSXa92NOXQYjIOEo6IKBglucwCU5rfNDZBJ/G8Eo+exxyaCmkBCTBwrwYKIsSkCfiCrRgZ5hnQc1BsAQlMSDHikcfb3oB4ZvPnPPojgmTHtGDZZOQos5MBoDmrxAjrkQRMZQCxGI0rrIiarIQdYjVJkDDgE3pYhhwgQS/B4YW42fNaAiSIGxjFxY4rrbz3c4COGQ5bMZiWLlBZApbniyTAQFqBJW9dGhK5AAXf9yixR4gRZzHeRJwISwurCw9w7uxHfkSQbJ8YYjYVQFIFUH5skGSAkUwDiAxi3tVI5VA2M5JEqeQMdJBYlJU8kQASD6phwWx7oZ2ZvTtf6RCCdsRgLLM/kSoQYy8W0LMYTdaST2Y7Yod0TawS7iO7cd7BUGnagKcOnCOhh/Cb3D8ZcRN7mvmd8e0811b5cx//EUGybZy03DVxjsnmjOKsUkNxcbggqwASWI374P5wvyNmIosYdSPHY2aXK63WydQZieImA5LAJsYkL7bT1w+1sgw1ZipmL2CwlACWgtA8TcBTCGzRePcghhAFXAfxZn6PRNqn546g3KInwRDGEglAcK8GirMCMABHBiB4iXsbaBE1BALJcf3uxY4q+vzNHxEkW4dnmWqunTtYsoESRdDhDDiDzed90A/0Xf+MHV3cJQHilgLKYLQKPDWR2WqOqORMJFSfAAmiY3w5SNBJAAkBNqi8ZESI7KLYZpqVhggpAuZUi2uFTfRkxM4Z0WHmSeM40qIzzY8AiTYzAQjSFJP5Jeg5iEhEGb6dCT0iiwRLzIKNg3opvXvljwiSLW/HmVlZbuqBgHKIrFJh0CsBCv930FT4rAPE7yGalGZKImYACAqrJxC7Kx5vZQIkdHiSSSqA5Hixv02IwLYwNzypAYghX41oKxSHdzF9wHNM+F+m2gA6CoPpZjGswqD7wPtrOyaWpfA5B4SDwvwxsWgOyjIAea+rpVdyX9+/2E6b/t5WeTP/8CPnuG56PWHDHwgoDJazig8gg+4ZbInBTrJGhYFPgKTK8/7bLloS/oYMQFzMYELipcRsvCwof9lMUkFxJdEHJgn5HeR0wB645Zf9NkR7yVbDwYajjdVy5rwijyTR0A3wqQAW86uQ6uheWPOrxPXEDHqFhWwxASkJBpjIrCcvuxWUY3Y5B4hch/QCUhi3PNtG2wefp3Ru9bahrb4zbdOrCVseOY9tf6hgYSB9UA/l6CDzI9/x13505uDoAHFHVMLfYM4nd2NjOSRBgpyPip+BpE+Y7QwGZi2D+sFJRuMM/nfPtrV0AHI5CPStf7y1WRDke6ADsLunN94zYAAJHYE8EpjH4jpjTzT9xfSYqADbtSxfNlpGlpQUleOo+3giEqAAeLAX4ACguWyI8HJ7S0X4dtDPVLRx1mGLGf9C9UGy8eWEs4dBgMojWDJu5mw9xQfSB/ZgR/988ugAqOyc/w+wJh1SLmJgEJxP7meIJiVmJEoiCiSy3ykdUcAMz4CkR3TPd7XMMABAjgcpADNubKr5dzW3BCByPtb+rZXWkT/yaGv7DAAiw55EIRKSCAYyiIBm/yy2zson4YjmyUgkIcVEpAxTwVpvh4QkfmPvGx0sO577gtk2DWivbUMuVtHGmlWorgFIBgb5joy35rM1CywHZZfkYPPaB5ojg30oLempBKjRxHUFNeO6jovm18wAABX8SURBVAxiYgbfRHJ5ApYEzrSol7jcd5Bg4Rib9DALBguHTDEGg/gN+R4TrmximWoTr2liyUcf3dhUM25pqlm3NdOcO5pp3p3NDFAkDC27r4UBipwUFGB8Lug4gMoA9WQbbXwqJicNaGNZ7yQfWXu6jYENwG18so19nu+RmL3qjy21+pH22vLm2bY/cXrfkUxf3DwoETdwTyGASYIlAZj9wJIEQ/J1BEWVMQv//QMds2Ia7sI2aybpnLos+CfMIRVBkq28YuGgYMImCacaFE9qIWyw/onWli4w6eoAkkn9mog2+dommnpdOE68uoklCo29orHGXN5Yoy5ppBEXN9Lwi0IbcfExGnlJI/3n0kb2/7FXhsSiD65urJx+TQTwJl1b3ng/sV9j5VzTWOP7NRaf4zujrmyiTf+5T2W71lSrgoCLmOSx+kyCCWwrynAVe5CpMrAwmIDFAeP6C+Ig2ZwBnAX4HqCL8QqulWmJGIbdQ+L6rnNkxEpikZO7se0YWWThxdFr6SBBLyF+E/NbXeS4ohh1k7LoeUX2kzE/v38zSzqCETw9kVV+ON22/qOdtjzXVhsHtNH6p9rom8fbaDUR5L+01ooHW2nJfa208N6WmndPS33Sv4VmZrVZd7bQ7Lta2P8X/KalPruvlZbdTxpja335UGutfri11jzaRt88xu+20ZbX+6h045TkONfodfVB8v1/y81IBiQ75yE5oPuJpMpYwMWWDziigZgGOkRizbBdK8Ywkq9Nz6gKEB7juCwoqu7adu+m6yTu1oZNKlNgHSgTeggrAisHjytph2SpI3p2v9LedAyURxZyWXKz6RNRvxh6bHnhPRTWWIPFvbXukCPDzP7nNVqSSiyWFmBNNsIGH1Av9kRTiIvGn6X07hU1Aod/ufogIcBnvoa4DDEzSD6AyUQZglQ++FUdYyDLIppx/a3PfB9MBtcqEyaqE/pnnB0yRxaoV+Lm9nPuBjeXN6vkWCGX8Gg6m6DEwiZJsZPT0wYEawKdAP0CgLivBKsDzyteWPwVONK8wUAMpoku/DBYUVyDRjyHVuE9vhMcZ9mN84nPuzVGmU5SLcmIm/cbpcv2+VhX+1h9kBSsCU4pHyRiIJmWDRyf4YmjMYS/9/WurkwmdAXXF/BjMJC29DGxHJJBTjY+n3x/oNe+jHJ+XFPLNWATPJ2kDODmxpmFEmtiJ+GBHd/dWMIsmUdaG0BgFgMAM9xd+cxuq9rUzSo2mbcV/QY/CL/NtfzZPFCHpUXjea2xzBMgxyWf/n+/X7PKzg6gwZcCiN8/SaVje0hbai52qg+Skl3Siv6JmV3V7KbCQKR7B1TyaAug46xnQA0UMV+Th6c5/XO0c8kaJ8Q8YtyDTrMW180m3yc73MFhg5K1zjYjcqLPxOMhPlNjOiN+DEQKFsn3L7U3PwUDYyAAIACBhmcVJxzswWuv6QrwYCsGHHYzhmRBeJIBk8xJ/3oU2PsroU/RL9wr9+lsQo7t7P41VmCrDxJKX63+2/6UjnPKKb2yY4WoZhIUB1gQDf06LUO7dAYdTAM4mRbjHgasxGsirQy+NdgigsmBYzMyEUgzoMT4STabRJAw6GTCsx4G55h5UD3SCxjMGsoCCQ6xCiChVknfkKrARErmiCQnkv2vkokGoOhPQM890x/0E7pTzO4vnniulL++2qKGL1YfJGz0s+Wd/akdayFJ8TADD+FUysDYDPdBS+gBzhg+e53moVBmJw8P7XPeZHhWvTK+l2wonwamLCABogxoEvfBOViJYzK4lm3pxIrLrKazuh/vds6AxBgDJRPlMsMiUQl1NuE5GEwCdhkmSSjXxixJFqkCIHyOvqZvmQg8uzPJ+ycZuxWOPVXprZ8cIZBw2V2flPsZMhQelUADQ4Id6HgTFXGGGyASK+d5QGS0s0ZGBwh1P0zGkmgMWHBw8X86BMA4aFzhcwDxe7QKwHEGOicsXbB7SoCGzrbyXZFJABnf5ze5JoCd3NvW5u4c2MGcWrZ9iimk5RbJ/tZKolK06SR9wr3RL/SdDXgUH9aX6FZMuIPkmdDPMCOg5j65Ry8bPryL9o3sodTGSUcQJPlrQrwmww6VgMJmZFaH+6DZIEZtnoFm0A0c7BcTs9RjUA1tHXc0MQqbofwf6mfQaHzPG79jrRIAJUHjLIOXNXOfDuKse8aS4B65xpRTLOZCnisr+TLlJzx/JFo2Zt2MPTGTOoBIMr0FlgHo/B73YAwWlVRee3Odiv5Fuc4wcpYexTodfgcgYzF5FHjIscofdpJSm6YeQZBQ+nvNk+UP5bPSFxfR+T4TAQa0zW4LPIyDwoHBQNNxDo6YLogJidVA8szet8KCaUzPTGe7guig4TdomKxJ8GSDpgJYXCwldBy79+gvsXtPWjkBJMRdcKET0CPxiHsFCBytYfqiPMZGMC+ju5DvCuC4D/oow7Kxz5J6lverg8ffA2y+6z4dfs93qojJRrnDe0k7PjuCIOHSW8dK1DXjxu2m40Ny4965DgoHhs94A0Z0exs4Ykg+pv0ZQEaG/WIIbuGHQAeA5snR4P8UAraIKfKfGeqWRQXQeE3ZyDZJwJivoQqRlGG8KG54Dr479RTzQwASAnzEWPC8EtUFEICBY8moEJklzkPjPYxoAEeBdTZJAtb7bGb2PSWAnOnbhIg2hgv7/3FtHHnoTLljzpGKdx1hkFhRvZuCX6ECa2QxBg/hbIGYYBDNAohVo13JixnpFHyBPdhICDEDQBgUrAnc4ITdOU/n28AAGExQVw7NVxHNUANMFE/QMc3zNRh0wAsAHDDJQeM15x3ofH5KBMkbHSxuQ5CO9b973uhgkVvC9TALR4vcxkgu53gmwG336bqJ3wP3YS2ha/k5v76/5zvJPp14sk0YWJd+oa++f6W9iuY/VCOA8OXqWzeZS6el1Y8HUeKzwGZclN8ZYPSO+9LFaGoEhSl4BozgnSzzGciuUsjUdzoZQMgEo+QDUVASfYi8kpKHPkCgDWahNAQDgLeznF28dLkDplcAqOkzlYgkHwSeIdkYFBrgQnEd09USj9BJWA9MPgmvuUdmMElJsAvgppEARA4KMxxxaUDBCgIo3IsBF5EbKxu52HRQ+5HP+iRjpWAUyzAUIPREI+5jx+DeSudWf6G4D3EtgETSns+kmefFWZAFDtuwEGCU54gGl3WQ3TAGNFwyIqb6DT3OZh/gsMItb3Qw1mAJAwD55tHWoiwDVYYYmE0D2mjny+3ts4CKAaDDLCHZ/Ba+nMHjHRE0DA73tp9Yikqwg8KPPmvRdSb1MvYCoFyfqkdz72imRb9ubvVDNjzZ2kpmkTfyXSxig97iSUeABYaEAU2ZNZEZ75PJU1mz2E2y8lEQaYg48k7oK8AJOMhE2/rqiSr+anS10hUdHH6sHZBQYWDlg1FR3D+bCzFgmr6Vcwq7UzDzjY7Zisw2LgyMAU1S2oEOpWMBB9FU6pSRM/HFA6FWGaH3ydc2MaCQzEPnMHNhFRdBBhQPoMVOzgwAoHX3uZfSBDAZ0LhYitaTz3D+PyEUtOF6AIF8ECoLTP9FUytqQy2Sz//Q0gCDRzbkh5RXQDK2GRiAjWiwICD78w1jj75KGuddfMX+Amgmgl+llEU7S2JCX1v3RFttfv0MFawar3Sq2Me5RsfaAQm3sHuRNP388hVr0XQloMWMYYbzoKC+nCU6WiYVgOBB6Twytuh4WIOkGgBARzMQn9/fwgq7fHJzSPChcMzU65sa3QOgUPqhQ7muEgvZAdBywES9xUFTYTFVQjQZyyTYhveuQ33Q3UxxBgkQk0/idUpGkx/St5ElH027oanm3hlSCGAbdBcSg7hXRBPPuf2Ftpao7OmNRJHpD1jB2sBQQgtxa/30cvtMJSQviQUINz7bWVvePE+7Zzyr0r3sal57O3rWHkjYq3bdW0qP72X5mkRCDRzoFpZ3ifgIKXYwhYMCOnZQoJRC1YgUAmfoHXQuOgjVgebd0UyzbgkVf6hVNvT8UIYKRqE0FSmDdBydCxABJYqiK7ZQu4m6hD8DlsNCMk/pfuIpWkzGOu5B7WHPh7/G17NA74D445uaGjhIPsqJSUYTrmys/17dxKohTbmuiT66oamm3dRU029pqhm3N9PsO5tp/q+b67PfttDy+8Pz8syr/tDSnv+rP7cSjb6wonn3t7DPz+rfTPPuba11r12knZP/pMIv31Oa3cl+gL9aBImkklylF96v4pEwRwRHkhaZBS+ShNPW5Cb0yCwEFKv/0kpf0REPtLDyl5SJIpFnzu3NLH/0o+ub6qPrmlpnT+7XRB9c0dhm7L/PPUbvnHeMGADyOrB8YCFAyEyHuaBwBtXBgtJY3rz2aoJtAI43W5UXgYTYjIvG0QWgfAAJ0D2FkXxXQJ1p97XQ4t80t/zXWTzLzU2N/SZc1djKZ8GGw/oeY21o32NEG3LBMRoSXw+78BgNu6iRhl4Yzr973jF6+9xj9NY5/6Phl7fU2pxXam1r16rwVbsg4SpF21Uy83blsjkyZqvLzH8GhQqaxbcA7drseKBlBhTURQUUM29qKkAx8aomep90v0saa+SFgcah8mEXNNLIi0nZa64Jt3bUxDu7acaDZ2jJH7uZUktlRCgYXQaa9ux0B4v5K6ir6o4uxJJXJLI9dyuCyAAVz6MU830zzW23ig4Gema/Z8wDUiYCE8JEKHoVSuyzbc1UZnLgW1n3BBllrfXVX1pp1Z9aavkDLfTZb0Odtbl3NbPCeZ/2byZvsM68WH+NWmzoPl883kOFG+ZXNb61cr72QYI+vW+L9k3vr23/Ch1IpyFGWIUPW1BNmVn/2d3NzSqYeXMzYwloetzljTWa3M++jTT4vMASMMXIy5ppSv8TNPuvZ2nxCzdozbCHtGPGa9q3Kkel25eZw6ho03x9/Y++Wvy/LUx/AYhclwFjxhtYBgcfBoCB7RBHNFbgwTaZhuMrNrO+4pIFY0jTrY61THUy1AEADIj+hOLt/ht+y3QxlNGMYhquiT+jxMx919dCbVZT4gehlIaSnJjSbMDEwnTTUV7tYMBHqee5mHR7Pvq90myK+QP9/SAg4V6Rj/vmP6F1Azrryz+3sk5kpmG+IkZgCyoJoluMuqiRVS8cBktc1FQjLm+h967vqDkPnaUv37pDGycN0PZF45W/bpFKd6+NW62nKu2S4h3rtO7f/bXwnlZW4ZAZjghC32FWow/BcMFnES0LX7YQnWBmZUSLwqwNLAtf0oDvhsVXb0fr4o0OxhDoJCixOM8AAGKJNTqm72REV8w3wULCpMYPY866hOPxI3wxiV0v+KwpzPhEQkYczAb4ABTK7LY3u6tsb83SASrtzHjyBwOJ/X6qRGWbZ2rbiOu18s+dNPeOsD7lv/2aaOyljTX6ksZ6v18LTbzjOE39zSla+PS1Wj/mce1eMkZF275UqmC30mXUIa0cEFU9WKpgj7ZOekrz7+2s2bc202Jo+YGWxmTMPJRbdBZmJkyQl3B2AR6UXgMCYIjFdjkPE+GPAGT4SBClVonxmbZmgTFggAkGwaLK6DVRObayFAy4Z6URysgE70ipyA7cJdIokks98NXk+FLOY7Xrzc4qWT22qu6o8fkfFiR+eyW5Kvz6fW0ado+WP3m2Fj7cR8te6Kc1w/6knZ++peJ105TOXS+lS/0bNT5Cv7uXTdKKpy/T9Bub27oYwAKzMOtRmPG9oLfAMCxoMtNzYFgwhdgAQLbgia3JKjNJX25v3wN4ZMkjAgATTBSUZFeKg1VlrnhiSwwyDEKsi/wVBwpHb5YIlQAJAT882rAPTANIopd176DOKlr2Yo37rKof+HFA4ldPFSm1Z63KdqyKOzgdHkP4zxzOsWTXRn075UXN+7+TA1huaWYbB2B9YGZiVWFdobugUJq3MlmdOSqfroTaEScfwIp+Hfwe+EBgKBxsiAGAAqO4coyIMBGENxW3OvvzMegMvkd1/ehpC5XFwhBTk3sbS6FHwXrUai1cPOBwuuWwPvvjguSwbq32PpxOp1S8/UttHveo5v/hFH18Syurrjz7tmZaeFeoxIy+hB8CxRqFFwceg4+VBOsYiP7extgHIMFANICFT4fvwyhmTaHzvBN3rcBXQ6ggxpXMH2Mix3cfT+glSf0ExiCOlAwL4PXFqTe+u+k9XAORueMV6o68VnsdlvVL9QIk5c+cVvH3q7V50r+06LFL9OHNbc3vwjreT25pprm3B5ZBLGF9sVSCwUdEYb3Ykkxf5/t48O+wOZIp5P/b3EQOdckQTeguzHL8NCGmFKLVFdgEsQEzOBA4JmNEgMKDeQArJ9RHsRr374bK1MZsr3RS6eaZ5Y9Zy6/qGUjKe69o1xZtX5SjlQPv1od3najx/Vro/biWF8Uax920XzQ1xsESw9OLD4cKAgTyAJE7/AjuzbktVBbAikI3QaFFn8lYUjH4iB5hbELcCFawCo0xdcItGQNEzxDoi4Vz8BYjuhBjKNDoTzjxYLVtb/dRat8P422lx+otSAwu6bRSxYUq3LZOm6e9o3kDbtHke3pp9NVtNLRvY9t5YugFwXnnGwy8d2ljsTB88jUBSICJwN4M4jR3NLMtTgCKO9O8cgDMAquExKRklLq8xhrmMisDAQRgsow8KycRY15vdzLgAcItzwRv9ZpHWmvvrAESiek/0F/9BklWp6ZTpSra9rW+nfUfrXz3Yc0bcKM+/P15GntrN428qp0G921iIQBc4zj78ASzKwWgIT6D34dosHt8AYvNdIJ4L7UzR5uJIfJfcK6ZQy9mr8WqSCEyHorg8FkztWNkHHDguMNjjQjE57ThlQtVtmtd1pPU7tsGkFTZn2mli/JUsH2Ddn2zTNtXztK3s0dr89RXtOGDx7RmyO+16vU7tOIf12nxk5dqwV/P1bw/nKalf+2lb57pqTWPdzCFFi/zusdjXZLnY8Q3RnnJZMOjChDsGE1tRAl+HKwnGMk91lhibOtGHZR5d7fRhkE3qvi7w6+BVuUjV/GPBpBU0TFVnyYEXybhBi8pUKooX6mCXJXu26OSvJ0qy9uh1L7vVbh6kraOuEXfPNtTqx/pYJFcSxN4rLXWPxlSBbCMAIA1apAMCJsfYVF5JBwnIBFu9sf57N6WWvrH47Vh0NXa98UopYtyq77NWvxPA0hqsTP3+6lUiUq//1x5S0dox+SHtXXEDdr0+jla93x3rX2qs77+axtLTkJ0wBBYUzRLh3ygtb56uIPWPnuSNr12vr77zx3aO2eAUt/NlMry97vUD3miASQ/ZO9W+O2UVLxHbL1avGWBitdPU8FXOcpfOU55y8Yqd8k47VkyTnuXjlPe8nHat2qCitdNVXr7PCn/G4lltUforwEkR6jj69JlG0BSl0brCN1rA0iOUMfXpcs2gKQujdYRutcGkByhjq9Ll20ASV0arSN0rw0gOUIdX5cu2wCSujRaR+heG0ByhDq+Ll22ASR1abSO0L02gOQIdXxdumwDSOrSaB2he20AyRHq+Lp02QaQ1KXROkL3+v8BikOdiwrHZsEAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## How To Succeed On the Labs\n",
    "\n",
    "Here are some simple tips that will help you do well on this lab:\n",
    "\n",
    "1.  Read/skim through the entire lab _before_ class.  If something confuses you, you can ask about it.\n",
    "2.  Start early.  Getting answers on edstem/piazza can take time.  So think through the lab questions (and your questions about them) carefully.\n",
    "    1. Go through the lab once (several days before the deadline), do the parts that are easy/make sense\n",
    "    2. Ask questions/think about the rest\n",
    "    3. Come back and do the rest.\n",
    "3.  Start early.  The DSMLP cluster gets busy and slow near deadlines.  \"The cluster was slow the night of the deadline\" is not an excuse for not getting the lab done and it is not justification for asking for an extension.\n",
    "4.  Follow the guidelines below for asking answerable questions on edstem/piazza.\n",
    "\n",
    "You may think to yourself: \"If I start early enough to account for all that, I'd have to start right after the lab was assigned!\"  Good thought!\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**The Cluster Will Get Slow** DSMLP and our cloud machines will get crowded and slow _before every deadline_.  This is completely predictable.  DSMLP can also get crowded due to deadlines in other courses.  You need to start early so you can avoid/work around these slowdowns.  Unless there's some kind of complete outage, we will not grant extensions because the servers are crowded.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Getting Help\n",
    "\n",
    "You might run into trouble while doing this lab.  Here's how to get help:\n",
    "\n",
    "1.  Re-read the instructions and make sure you've followed them.\n",
    "2.  Try saving and reloading the notebook.\n",
    "3.  If it says you are not authenticated, go to the [the login section of the lab](#Logging-In-To-the-Course-Tools) and (re)authenticate.\n",
    "4.  If you get a `FileNotFoundError` make sure you've run all the code cells above your current point in the lab.\n",
    "4.  If you get an exception or stack dump, check that you didn't accidentally modify the contents of one of the python cells.\n",
    "5.  If all else fails, post a question to edstem/piazza.\n",
    "\n",
    "## Posting Answerable Questions on Edstem/Piazza\n",
    "\n",
    "If you want useful answers on edstem/piazza, you need to provide information that is specific enough for us to provide a useful answer.  Here's what we need:\n",
    "\n",
    "1.  Which part of which lab are you working on (use the section numbers)?\n",
    "2.  Which problem (copy and paste the _text_ of the question along with the number).\n",
    "\n",
    "If it's question about instructions:\n",
    "\n",
    "1.  Try to be as specific as you can about what is confusing or what you don't understand (e.g., \"I'm not sure if I should do _X_ or _Y_.\")\n",
    "\n",
    "If it's a question about an error while running code, then we need:\n",
    "\n",
    "1.  If you've committed anything, your github repo url.\n",
    "2.  If you've submitted a job with `cse142` you _must_ provide the job id.  It looks like this: `544e0cf2-4771-43c3-86f8-1c30d7af601f`.  With the id, we can figure out just about anything about your job.  Without it, we know nothing.\n",
    "3.  The _entire_ output you received.  There's no limit on how long an edstem/piazza post can be.  Give us all the information, not just the last few lines.  We like to scroll!\n",
    "\n",
    "For all of the above **paste the text** into the edstem/piazza question.  Please **do not provide screen captures**.  The course staff refuses to type in job ids found in screen shots.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**We Can't Answer Unanswerable Questions** If you don't follow these guidelines (especially about the github repo and the job id), we will probably not be able to answer your question on edstem/piazza.  We will archive it and ask you to re-post your question with the information we need.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Keeping Your Lab Up-to-Date\n",
    "\n",
    "Occasionally, there will be changes made to the base repository after the\n",
    "assignment is released.  This may include bug fixes and updates to this document.  We'll post on piazza/edstem when an update is available.\n",
    "\n",
    "In those cases, you can use the following commands to pull the changes from upstream and merge them into your code.  You'll need to do this at a shell.  It won't work properly in the notebook.  Save your notebook in the browser first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "First, you need to run the `./fix-repo` in the cell at the top of this section.  After that you can run  \n",
    "\n",
    "```\n",
    "./pull-updates\n",
    "```\n",
    "\n",
    "at a terminal.\n",
    "\n",
    "Then, reload this page in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Editing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "For programming assignments, it can be nice to use an editor other than the jupyter notebook editor.  Below are some student-provided instructions for using various editors with datahub.  These are supported, but if you provide feedback about them, we'll try to update and improve them.\n",
    "\n",
    "### VSCode\n",
    "\n",
    "1. Install the “SSH-Remote” extension in VS Code\n",
    "2. Open the Command Palette in VS Code\n",
    "3. Type “remote-ssh” and click “Remote-SSH: Connect current window to host”\n",
    "4. It should ask you to put in the command: ssh {your_username}@dsmlp-login.ucsd.edu\n",
    "5. If it asks for fingerprint authorization or something similar, accept\n",
    "6. It should then ask for your password (your ucsd.edu email password)\n",
    "7. Once logged in, click \"Open Folder\" in the file explorer sidebar\n",
    "8. Open the folder pertaining to the lab you are working on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## How To Use This Document\n",
    "\n",
    "You will use Jupyter Notebook to complete this lab.  You should be able to do much of this lab without leaving Jupyter Notebook.  The main exception will be some of the programming assignments.  The instructions will make it clear when you should use the terminal.\n",
    "\n",
    "### Logging In\n",
    "\n",
    "If you haven't already, you can go to [the login section of the lab](#Logging-In-To-the-Course-Tools) and follow the instructions to login into the course infrastructure.\n",
    "\n",
    "### Running Code\n",
    "\n",
    "Jupyter Notebooks are made up of \"cells\".  Some have Markdown-formatted text in them (like this one).  Some have Python code (like the one below).\n",
    "\n",
    "For code cells, you press `shift-return` to execute the code.  Try it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"I'm in python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Code cells can also execute shell commands using the `!` operator.  Try it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!echo \"I'm in a shell\""
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Telling What The Notebook is Doing\n",
    "\n",
    "The notebook will only run one cell at a time, so if you press `shift-return` several times, the cells will wait for one another.  You can tell that a cell is waiting if it there's a `*` in the `[]` to the left the cell:\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You'll can also tell _where_ the notebook is executing by looking at the table of contents on the left.  The section with the currently-executing cell will be red:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAADcAAAA1CAYAAADlE3NNAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAycDPIMYgwyCSmFxc4BgQ4ANUwgCjUcG3awyMIPqyLsisHbud/s7f09wx77xWn4rBr1hM9SiAKyW1OBlI/wHitOSCohIGBsYUIFu5vKQAxO4AskWKgI4CsueA2OkQ9gYQOwnCPgJWExLkDGTfALIFkjMSgWYwvgCydZKQxNOR2FB7QYDHxdXHRyHAxNjQPJCAc0kHJakVJSDaOb+gsigzPaNEwREYSqkKnnnJejoKRgZGhgwMoDCHqP75FhyWjHWbEWKJwQwMhq1AQSGEWLYoA8Oe3wwMQrsRYlp5DAyCDQwM++MLEosS4Q5g/MZSnGZsBGGLhzEwcHb9//8C6FHuf0C79P7//8H7///vegYG9iIGhm4rACicXg0u2h90AAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAA3oAMABAAAAAEAAAA1AAAAABCNvQ0AAAJGSURBVGgF7VkxayJBFP52XVC4YHNRxFoQRLSwvLTaWopgArkfYK2IlUViY5PzL1jZqI2dlbbaiWisREXUQnJYyF6S2WLvAgvZ3cwes8NMNTvz9r3ve9+beSwrnc/nV3A6ZE55abQEObeqK5QTyjGYAWW/3zMIiw4k6fV90HHFnhdxobCniTlEQjlzeWLPShmNRuyhooWI3JZWxnA4tGJOxdZuTHHmaFXJ//bDtXKKE9ncbDbYbremXMdiMXi9XlO2Vox2ux0cIVcqlTAej01hqVQqyOVypmw/M7pcLmg2m+h0OjgcDs6QU1X1Mxz6vqr+0edfmby8/Mbd3S0Wi4VWCfF43BlyXwFp993HxweNWCqVQr1eRzAYhJzJZJBIJEDKw0rG7YJw4r31eo1ut4tAIICnp18aMRJHbjQaqNVq6PV6GAwGTsR23OdkMtFipNNpXF190+PJpDaz2SwikQhWq5W+4aYJUY6McDj8Abbe53w+n2vLMhqNaqRms5kxuQ+rLntIJpMg4vT7fSyen3X0unL6igsnfr8fxWIRpM/9vL/X+hxpDVyQI3oUCgXk83mcTidUq1Xc3Pz42+darRY1zUKhEJbLpSl/19ffTdmZMSqXyyCtrd1uYzqdAlY/uOx+W1mN86+93ZjclKWRsoKcUVbcsCaUc4NKRhglchMZbfCwJn6EuFVFcaEI5RjMgDKfzxmERQeSdDweuW0Fiizze6coHo+HTg0w6IVv5bguS67JSZLE4GmhA4nrM8dvH3gXX+G5LN8AO2tDVwI9BI4AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### What to Do Jupyter Notebook It Gets Stuck\n",
    "\n",
    "First, check if it's actually stuck: Some of the cells take a while, but they will usually provide some visual sign of progress.  If _nothing_ is happening for more than 10 seconds, it's probably stuck.\n",
    "\n",
    "To get it unstuck, you stop execution of the current cell with the \"interrupt button\":\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC4AAAAiCAYAAAAge+tMAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAxcDBIMjAw6CZmFxc4BgQ4ANUwgCjUcG3awyMIPqyLsisycd66uYamIqqf/v/Otp99zlM9SiAKyW1OBlI/wHitOSCohIGBsYUIFu5vKQAxO4AskWKgI4CsueA2OkQ9gYQOwnCPgJWExLkDGTfALIFkjMSgWYwvgCydZKQxNOR2FB7QYDHxdXHRyHAxNjQPJCAc0kHJakVJSDaOb+gsigzPaNEwREYSqkKnnnJejoKRgZGhgwMoDCHqP75FhyWjHWbEWKJwQwMhq1AQSGEWLYoA8Oe3wwMQrsRYlp5DAyCDQwM++MLEosS4Q5g/MZSnGZsBGGLhzEwcHb9//8C6FHuf0C79P7//8H7///vegYG9iIGhm4rAKtIXcvOgP2JAAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAAuoAMABAAAAAEAAAAiAAAAALSq6z8AAAJiSURBVFgJ7Zi/ayJBFMe/HoLCiZ0WdrERRFDwHxBBazstPCHXCBexUHJgIZYXW3P+BzaCIFpZWSgKFwQtDCqaRsHCH4WQEyXqXd6AksQznM6uENgBnTdvmPc++93Zt8PK/jw3fMD26QMyM2QJ/Nx3Tl6tVs+d82A+hUKB5XJ5cP7VBD2ch1qlUjk0JYr/mHzSHn91G88wkBTfijwajdDv97dDQfvFYoH1es1iyoWI/PT0hGQyiVwuh+l0ykKqVCo4HA5cX3/H3d0vjMdjuN1urnT1eh2pVAqJRALc4I+Pv+HzfUGv1wOVM5PJBKVSiXa7jWw2i1KphNlsBpvNxg1OV10ulxEMBvnBb25+MGir1Yp4PA6tVstUpQu6uvoGUknoRvBcig+HQ+TzeWg0Gtze/oRK9XnHeH/fRLPZ3I2FNrjAG40G46G9/BKanHQxgUBgx2uxWHa2EAYXOClOTafT7bHo9XrQT6zGVccNBgPj6nQ6YvEdjMsFbjabWQUpFAroPTzsJZlMJqjVant+IRxc4Gq1mpUmquNfLy9ZHadqMp/PUSwW4fF44Pf7sX0WhACmGDKZjK+qUBCv14vBYIB0Oo1oNIpYLEZubDYb1rtcLhiNRmYL8UfQoVCIH5xgIpEInE4nMpkMWq0WVqsVexHZ7XbmFwKYYtALLhwOP7/wfMB7B+tjzsfvxfnfuWPyce1xoZQ8JY4EfopqPGskxXnUO2WtjJ7kUxaKseaYzxMyKlViQIgdU9rjYiv8Nr6k+FtFxB7Lu92u2Dn+Gf9lTSB7O97a29Ml9fQthfxkX1xcsHh/AeSdPWhpR3GTAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You can also restart the underlying python instance (i.e., the confusingly-named \"kernel\" which is not the same thing as the operating system kernel) with the restart button:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Once you do this, all the variables defined by earlier cells are gone, so you may get some errors.  You may need to re-run the cells in the current section to get things to work again.\n",
    "\n",
    "You can also try reloading the web page.  That will leave Python kernel intact, but it can help with some problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Common Errors and Non-Errors\n",
    "\n",
    "1.  If you get `sh: 0: getcwd() failed: no such file or directory`, restart the kernel.\n",
    "2.  If you get `INFO:MainThread:numexpr.utils:Note: NumExpr detected 40 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.`.  It's not a real error.  Ignore it. \n",
    "3.  If you get a prompt asking `Do you want to cancel them and run this job?` but you can't reply because you can't type into an output cell in Jupyter notebook, replace `cse142 job run` with `cse142 job run --force`. (see useful tip below.)\n",
    "4.  If you get an `Error: Your request failed on the server: 500 Server Error: Internal Server Error for url=http://cse142l-dev.wl.r.appspot.com/file`, trying running the job again.\n",
    "5.  Sometimes `cse142 job run` will just sit there and seemingly do nothing.  Weirdly, interrupting the kernel (button above) seems to jolt it awake and cause it to continue.\n",
    "6.  If you get an error like this, stop your datahub server and restart it.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "7. If you get `http.cookiejar.LoadError: '/home/youruserrname/.djr-cookies.txt` does not look like like a Netscape format cookies file.` remove the file and re-authenticate.\n",
    "8.  The table of contents disappears and/or the questions are not highlighted like they usually are.  Do this:    \n",
    "    1.  Go to the file browser in jupyter\n",
    "    2.  At the top, there is tab labeled \"Nbextensions\".  Click on it.\n",
    "    3.  find \"Table of Contents (2)\".  It should be checked.  Un check it, and check it again.\n",
    "    4.  Click the \"refresh button\" (circular arrows)at upper right.\n",
    "    5. Reload your notebook.\n",
    "9.  You produce too much output from a program and your notebook refuses to open because it's too big.  Try\n",
    "    1.  Backup up your notebook!\n",
    "    2.  This will work, but it will clear _all_ your output: https://stackoverflow.com/a/47774393/3949036\n",
    "    3.  You can open the notebook file in a text editor and remove the output manually.\n",
    "    "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAADkAAAApCAYAAACPzoEeAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAySDFwMOgyGCemFxc4BgQ4ANUwgCjUcG3awyMIPqyLsgsNhlLvoOd7mlp6cLz5mlnTsVUjwK4UlKLk4H0HyBOSy4oKmFgYEwBspXLSwpA7A4gW6QI6Cggew6InQ5hbwCxkyDsI2A1IUHOQPYNIFsgOSMRaAbjCyBbJwlJPB2JDbUXBHhcXH18FAJMjA3NAwk4l3RQklpRAqKd8wsqizLTM0oUHIGhlKrgmZesp6NgZGBkyMAACnOI6p9vwWHJWLcZIZYYzMBg2AoUFEKIZYsyMOz5zcAgtBshppXHwCDYwMCwP74gsSgR7gDGbyzFacZGELZ4GAMDZ9f//y+AHuX+B7RL7///H7z///+uZ2BgL2Jg6LYCAM3XWvMCZhWeAAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAA5oAMABAAAAAEAAAApAAAAAPS5PxgAAALOSURBVGgF7Vm9j2lBFD/uXh95u+sjm6DZrChUClohehXJJhvxJ0hESUcnGiVbicoWQiNEoZFQqHw0NArFKhDsCpENnpmE7MaVd3nD+jqJ3LnnzPzOOXPOnJlxOePxeA5nTtSZ+4fduwgn6U6nc/bB5MwXdO5eXkS6Xp08lzS+iEjShUKBVcC4XC48PT2BVCpd6//5+Qnv7+/Q7XbXZEfBQNWVDX18fMyLxSJj11qtNu/1eowyUsx8Pr8zFOt0vb+/h6+vL8bAoAiKxWJG2TEwWTt5DMbuasOvODkYDGA0Gu1q89bjDupks9kEm80GBoMBdDod2O126Pf7WxvNZgCaxHq9DpPJBGg2A0j0mU6n4HA4AFVip9MJw+EQ3t7ewOVyQSgUIqFihREIBCASicBsNgOBQAB0PB7HQo1GA0qlctWRdKNarUKj0QCfzwcmkwnDo20pGAwCSl+RSEREZSwWg3A4DBaLBYxGIyQSCaArlQoGf319hUwmQ0QRE0ir1cJslKZLslqtOF1vbsglVDqdBpVKBV6vF6vR6/VAKRQKcLvdOI2WyvfxXGxya7Bo20G67+5u12S7MtrtNqjV6tVwHo8HdDQaBbPZvGLuo4GKQCqVwtB+vx+Q4u8kkUjweuVwON/ZW7ez2Sw+dZVKJfB4PKvx9OKksnrZV6NcLkMul8PwyWSSUc3LywvI5XJGGVsmWouosKEfWv9LolAhEAqFy/dfezKlMyljKFR9RqMxKbyjxKHQwr+9/XOUxpEyin5+fsZYWq2WFOYajkwmAz6fj08fa8IFAy0XEvvk4+MjLLfEH3q2ub9suu5s4m+D/a++/6PjoGfXH7N7wJerkwec7L2qukZyr9N7QHDWkURHJXQ1YqKHh4e9XX6Z9G3L46DSzGbQKf8lef3gwybCp9CH9Zo8BWc22Xh1ctPMnBqfPtqPNARnkqao88/Yi3DyL0l+mHXQ5TN9AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Useful Tips\n",
    "\n",
    "1.  If you need to edit a cell, but you can't you can unlock it by pressing this button in the tool bar (although you probably shouldn't do this because it might make the lab work incorrectly.  A better choice is to copy and paste the cell, _and then_ unlock the copy):\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAACEAAAAkCAYAAAAHKVPcAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAycDPIMYgwyCSmFxc4BgQ4ANUwgCjUcG3awyMIPqyLsisHbud/s7f09wx77xWn4rBr1hM9SiAKyW1OBlI/wHitOSCohIGBsYUIFu5vKQAxO4AskWKgI4CsueA2OkQ9gYQOwnCPgJWExLkDGTfALIFkjMSgWYwvgCydZKQxNOR2FB7QYDHxdXHRyHAxNjQPJCAc0kHJakVJSDaOb+gsigzPaNEwREYSqkKnnnJejoKRgZGhgwMoDCHqP75FhyWjHWbEWKJwQwMhq1AQSGEWLYoA8Oe3wwMQrsRYlp5DAyCDQwM++MLEosS4Q5g/MZSnGZsBGGLhzEwcHb9//8C6FHuf0C79P7//8H7///vegYG9iIGhm4rACicXg0u2h90AAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAAhoAMABAAAAAEAAAAkAAAAAP1oLUoAAAHGSURBVFgJ7VZdq4JAEB0/+jIIqocg9LH//4sCjYosKIvMitLLEfbSLce7q0Y9eEAWd3dmzp7ZnV1ts9kk9GHoH46fhq9JiCzUStRKCAVE+xV7whRsuHa/39Nut6Pz+UxJolbXDMMgy7JoNBpRo9HgQlAuCd/3abVascYyA1EUERYymUyo2WxmmrDpuFwuBBJV4Ha70Xw+Z12xShyPxz/yDwYD0nWWc2aA0+lE+IAwDDPnoJMlAfYCpmmS4zjiV7rFXprNZun8OI5ZO7WlsW7KDZQigdW5rkvb7bYUCzYdMl4Xi0W68w+HA7Xb7fQ4ytg9zymsRBAEvwqgfnieR3l5fw78+F+IxPV6fTly6IMyRVCIBHb8/X5/iYe9AYVUoUxivV7nnnkUJaiiAiUScP5fGYdCy+VShQNfrDgvnU6HHgtZ1rxWq5XVzfYpHVFcQLiIqoZSOqoOLvxJkVB9Rwjnsi2bjse7H5ttOp0q36J4CAnggcOBJdHtdtOgogriai+DXq/HmrPpgBK2bZOmaayx7ABOy3g8ZqezSsCi3++nFxPeBXhpqQIpwJEeDoe5qcwlgaBwgu+dYNPxzqDPvmsSQpFaCaHEDz84qm5DUiF+AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### The Embedded Code\n",
    "\n",
    "The code embedded in the lab falls into two categories:\n",
    "\n",
    "1.  Code you need to edit and understand.\n",
    "2.  Code that you do not need to edit or understand -- it's just there to display something for you.\n",
    "\n",
    "For code in the first category, the lab will make it clear that you need to study, modify, and/or run the code.  If we don't explicitly ask you to do something, you don't need to.\n",
    "\n",
    "Most of the code in the second category is for drawing graphs.  You can just run it with shift-return to the see the results.  If you are curious, it's mostly written with `Pandas` and `matplotlib`. The code is all in `notebook.py`.   These cells should be un-editable.  However, if you want to experiment with them, you can copy _the contents_ of the cell into a new cell and do whatever you want (If you copy the cell, the copy will also be uneditable).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Most Cells are Immutable** Many of the cells of this notebook are uneditable. The only ones you should edit are some of the code cells and the text cells with questions in them.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Pro Tip** The \"carrot\" icon in the lower right (shown below) will open a scratch pad area.  It can be a useful place to do math (or whatever else you want.\n",
    "    \n",
    "![image.png](attachment:image.png)\n",
    "</div>"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Showing Your Work\n",
    "\n",
    "Several questions ask you to show your work for calculations.  We don't need anything fancy.  Many of the questions ask you to compute something based on results of an experiment.  Your experimental results will be different than others', so your answer will be different as well.\n",
    "\n",
    "To make it possible to grade your work (and give you partial credit), we need to know where your answer came from.  This why you need to show your work.  For instance this would be fine as answer to \"On average, how many weeks do you have per lab?\":\n",
    "\n",
    "```\n",
    "Weeks in quarter/# of labs = 10/5 = 2 weeks/lab\n",
    "```\n",
    "\n",
    "2 significant figures is sufficient in all cases, but you can include more, if you want.\n",
    "\n",
    "If you are feeling fancy, you can use LaTex, but it's not at all required.\n",
    "\n",
    "When it's appropriate, you can also paste in images.  However, Jupyter Notebook is flaky about it.  Save your notebook by clicking the disk icon:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Answering Questions\n",
    "\n",
    "Throughout this document, you'll see some questions (like the one below).  You can double click on them to edit them and fill in your answer.  Try not to mess up the formatting (so it's easy for us to grade), but at least make sure your answer shows up clearly.  When you are done editing, you can `shift-return` to make it pretty again.\n",
    "\n",
    "A few tips, pointers, and caveats for answering questions:\n",
    "\n",
    "1. The answers are all in [github-flavored markdown](https://guides.github.com/features/mastering-markdown/) with some html sprinkled in.  Leave the html alone.\n",
    "2. Many answers require you to fill in a table, and many of the `|` characters will be missing.  You'll need to add them back.\n",
    "3. The HTML needs to start at the beginning of a line.  If there are spaces before a tag, it won't render properly.  If you accidentally add white space at the beginning of a line with an html tag on it, you'll need to fix it.\n",
    "4. Text answers also need to start at the beginning of a line, otherwise they will be rendered as code.\n",
    "5. Press `shift-return` or `option-return` to render the cell and make sure it looks good.\n",
    "6. There needs to be a blank line between html tags and markdown.  Otherwise, the markdown formatting will not appear correctly.\n",
    "\n",
    "\n",
    "You'll notice that there are three kinds of questions: \"Correctness\", \"Completeness\", and \"Optional\".  You need to provide an answer to the \"Completeness\" questions, but you won't be graded on its correctness.  You'll need to answer \"Correctness\" questions correctly to get credit.  The \"Optional\" questions are optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Logging In To the Course Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "    \n",
    "In the course you will use some specialized tools to let you perform detailed measurements of program behavior.  To use them you need to login with your `@ucsd.edu` email address using the instructions below. **You need to use the email address that appears on the course roster.  That's the email address we created an account for.  In almost all cases, this is your `@ucsd.edu` email address.**\n",
    "\n",
    "You'll do this periodically when you get an error about not being authenticated.  You can return to this notebook (or any other of the lab notebooks) to login at any time.\n",
    "\n",
    "Here's what to do:\n",
    "\n",
    "1.  Enter your `@ucsd.edu` email address in quotes after `login` below.  It'll take a few seconds to load.\n",
    "2.  Click the google \"G\" login button below and login with your `@ucsd.edu` email address. \n",
    "3. **Click the google button regardless of whether it says \"sign in\" or \"signed in\".  Then be sure to select your `@ucsd.edu account` if it shows you multiple google acocunts**\n",
    "4. You'll see a very long string numbers an letters appear above.  Click \"Copy it\" to copy it.\n",
    "\n",
    "**Note:** If it doesn't give you a choice about which account to log into and authentication fails, that means you are logged into a single Google account and that account is _not_ your `@ucsd.edu` account.  You'll have to log into your `@ucsd.edu` through Gmail or through Chrome's account manager and then try again.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Use Chrome** The login process doesn't seem to work properly with Safari or Firefox.  Use Chrome to login.  You can use any of the other compatible browsers you want for the doing the rest of the lab, and it should be fine.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "login(\"<Your @ucsd.edu email address>\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Next step:  Paste it below between the quote marks.  Press `shift-return`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cse142L.is_response": true,
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "token(\"your_token\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "It should have replied with\n",
    "\n",
    "``` \n",
    "You are authenticated as <your email>\n",
    "```\n",
    "\n",
    "You are now logged in!  Try submitting a job: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!cse142 job run \"echo Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "If you see \"Hello World\", you're all set.  Proceed with the lab!\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "Delete your token from the above cell. Because your token is esssentially your username and password combined, you should treat it like a password or ssh private key.  **Sharing your token with another student or posessing another student's token is an AI violation**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Your grade for this lab will be based on the following components\n",
    "\n",
    "| Part                       | value |\n",
    "|----------------------------|-------|\n",
    "| Reading quiz               | 3%    |\n",
    "| Jupyter Notebook           | 45%   |\n",
    "| Programming Assignment     | 50%   |\n",
    "| Post-lab survey.           | 2%    |\n",
    "\n",
    "No late work or extensions will be allowed.\n",
    "\n",
    "We will grade 5 of the \"completeness\" problems.  They are worth 3 points each.  We will grade all of the \"correctness\" questions.\n",
    "\n",
    "You'll follow the directions at the end of the lab to submit the lab write up and the programming assignment through gradescope. \n",
    "\n",
    "Please check gradescope for exact due dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Temporal Locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "In the last lab, we examined the notion of spatial locality in detail.  Now, we will turn to temporal locality.\n",
    "\n",
    "Temporal locality exists when a program accesses the same memory multiple times within a short time.  Caches exploit temporal locality by holding on to data that has been accessed recently.  If the processor accesses it again, the cache can provide it very quickly.\n",
    "\n",
    "With spatial locality, it was pretty easy to predict the cache miss rate for a simple loop that performs stride-based accesses (see below).  With temporal locality it is harder because of associativity and conflicts.  Before we dive into that, let's have quick refresher about how caches work (if this is fuzzy, go back and the review the slides and/or readings).\n",
    "\n",
    "When a memory operation (load or store) accesses a memory location, $A$, the cache breaks $A$'s address into three parts: \n",
    "\n",
    "| tag | index | offset | \n",
    "|-----|-------|--------|\n",
    "| the remaining bits | `log2(# of associative sets)` | `log2(cache line size)`|\n",
    "\n",
    "Together, the tag and the index of $A$ are a unique name (or number) for the cacheline-sized (and cacheline size-aligned) piece of memory that contains $A$.  The index of $A$ tells that cache which associative set might contain that cache line.\n",
    "\n",
    "The cache can then check that set to see if $A$ is present.  If it is, it's a hit.  If not, it's a miss, and the cache will choose one of the lines in the set to evict to make room for $A$'s cache line.\n",
    "\n",
    "There are two important things to note:\n",
    "\n",
    "1.  $A$'s cacheline is in the cache if and only if, it is in the associative set corresponding to its index (it can never be in another associative set).\n",
    "2.  There are many, many other cache lines that also \"live\" in $A$'s associative set.\n",
    "\n",
    "The L1 data cache in our processor is 32kB, with 64-byte lines, and it's 8-way set associative. So, there are 32,768/64 = 512 cache lines arranged in 512/8 = 64 associative sets.  If the machine has 16GB of memory, it has 256-Million cache lines of main memory.  So, there are about 4 million cache lines that \"live\" in each associative set.  Clearly, there is plenty of opportunities for conflicts.\n",
    "\n",
    "To see how temporal locality plays out in practice, here's the same code we looked at in the last lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"stride.cpp\", function=\"stride\", name=\"spatial\", opt=\"-O1\",\n",
    "code=r\"\"\"\n",
    "#include\"pin_tags.h\"\n",
    "#include\"CNN/tensor_t.hpp\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* stride(uint64_t * data, uint64_t size, uint64_t arg1) {\n",
    "    tensor_t<uint32_t> t(size,1,1,1, (uint32_t *)data);\n",
    "    TAG_START(\"init\", t.data, &t.as_vector(t.element_count()), true);\n",
    "\n",
    "    for(uint i = 0; i < arg1; i++) {\n",
    "        for(uint x = 0; x < size; x+=arg1) {\n",
    "            t.get(x,0,0,0) = x;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    TAG_STOP(\"init\");\n",
    "    return data;\n",
    "}\n",
    "\n",
    "FUNCTION(one_array_1arg, stride);\n",
    "\"\"\")\n",
    "compare([t.source, t.cfg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We are going to run it again with a fixed stride of 16 elements (64 bytes -- our cache line size) and we will vary `size` between 1024 and 16,384 (16 * 1024).  This corresponds to region of memory between 4kB and 128kB.  Setting the stride to the cache line size ensures that our access stream has very little _spatial_ locality, since every access will refer to a different cache line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Given the conditions described above, estimate the _number of cache misses_ that will occur for `size = 1024`, `size = 4096`, and `size = 16384`.  Assume we run `stride()` 10,000 times with the same values of `data` and `size`.\n",
    "\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "**Cache misses for size = 1024:**\n",
    "\n",
    "**Cache misses for size = 4096:**\n",
    "\n",
    "**Cache misses for size = 16384:**\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "With `size = 1024`, the data occupies 4kB or 64 cache lines.  The first time we access each cache line in the inner loop of `stride`, we can expect a cache miss, so that's 64 misses.  Notably, each of these will land in a _different_ associative set.  After those 64 misses, however, there shouldn't be any more:  Our code doesn't access any other memory, so those 64 cache lines should remain in the cache. \n",
    "\n",
    "With `size = 4096`, the data occupies 256 cache lines, which should result in 256 misses.  This time, we will use $256/64 = 4$ cache lines in each associative set. There's still plenty of room left in the cache, so we shouldn't expect any more misses.\n",
    "    \n",
    "Moving to `size = 16384` will change things:  Now we have 1024 cache lines, which is twice as many as our cache can hold.  On the first iteration of the inner loop, we will take a cache miss and bring a cache line into the cache at some index, $N$.  During the first 511 iterations of the inner loop, we will touch 511 more cache lines, incurring a total of 512 misses, and occupying all 8 cache lines in each of the 64 associative sets.  On the 513th iteration, the cache will have to evict a line at index $N$.  Then, over the next 511 iterations we will evict _everything_ in the cache, incurring a total of 1024 cache misses. \n",
    "\n",
    "During the next iteration of the outer loop (`arg1`) will repeat this process 16 times for `16*1024 = 16,384` misses per call to `stride`.   We call `stride` 10,000 times for `16,384*10,000 = 163,840,000`  cache misses.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run the cells below to see how your prediction played out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"stride.cpp\", function=\"stride\", name=\"temporal\", run=[\"perf_count\"], tagged_only=True, opt=\"-O1\", \n",
    "           cmdline=f\"--size 1024 2048 4096 8192 16384 32768 --arg1 16 --iters 10000\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "# compare([t.source, t.cfg]) # Uncomment this line to see the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(render_csv(\"temporal.csv\", columns=[\"function\",\"size\", \"arg1\", \"IC\", \"CPI\", \"L1_MPI\",\"L1_cache_misses\"]))\n",
    "plotPE(\"temporal.csv\", lines=True, what=[ ('size', 'CPI'), (\"size\", 'L1_MPI')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "#### How well do your predictions match the results?\n",
    "\n",
    "<div class=\"answer\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "For small sizes, agreement is not great.  We predicted 64 misses at `size=1024` and 256 at `size=4096` but we have a few hundred. This is fine, there's other stuff going on and the L1_MPI is still very, very low.\n",
    "    \n",
    "Our prediction at `size = 16384` is much better. \n",
    "    \n",
    "The `size=8196` is interesting. It should exactly fit, but we have a lot more misses.  This is because there's other stuff in memory (e.g., the stack) and the tensor should just barely fit.   However, while get ~1000x more misses than at `size=4096`, `L1_MPI` is still quite low and `CPI` doesn't change at all.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Moneta has a simple cache simulator built into that can estimate how a cache will behave and let us visualize the results.  It models a fully-associative cache.  Run the next two cells to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"stride.cpp\", function=\"stride\", name=\"temporal\", run=[\"moneta\"], tagged_only=False, opt=\"-O1\", \n",
    "           cmdline=f\"--size 1024 4096 16384 --arg1 16 --iters 1\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500 --calc misses_per_iter=L1_cache_misses/iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "show_trace(\"./temporal_0.hdf5\", show_tag=[\"init0\",\"init1\",\"init2\"], layer_preset=[\"misses-compulsory-all\", \"misses-all\", \"hits-all\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    " In the plots, compulsory misses are light blue, hits are purple, and capacity misses are gold.\n",
    " \n",
    " The green line on the y-axis shows the size of the L1 cache (32kB), and the large blocks are the accesses for `size = 1024`, `size = 4096`, and `size = 16k`. \n",
    "\n",
    "In each block, each upward-slanting line is one iteration through the outer loop (they are probably blurred together for 1024), and if you zoom in you can see that the stride is 64 bytes (try it!).  As you can see, when the region of memory is larger than the cache, the whole block turns gold instead of purple.\n",
    "\n",
    "This is a good chance to use Moneta's measurement tool: Click on the ruler icon above the Moneta graph and use it to select the the middle block of accesses.   A display on the right will open up with some statistics about that region of the plot:\n",
    "\n",
    "![Measuring with the ruler tool](img/measuring.gif)\n",
    "\n",
    "The \"Cache Measurement\" heading provides information about the cache that Moneta is simulating.  Under \"current selected area\", you can see that the selected area contains accesses to 256 distinct cache lines.  You can also see the cache miss rate for that area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "    \n",
    "How many distinct cache lines are accessed with `size = 16384`? Use the measurement tool to check.\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"answer\">\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\"> \n",
    "\n",
    "I got 1021, but it's probably actually 1024.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's try another experiment and increase the stride by 4x to 64 element (256 bytes) while using `1024`, `4096`, and `16384` for `size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "How will increasing the stride to 64 affect the number of misses when `size = 16384`?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\"> \n",
    "    \n",
    "Let's see what happens:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "t = fiddle(\"stride.cpp\", function=\"stride\", run=[\"moneta\"], tagged_only=False, opt=\"-O1\", \n",
    "           cmdline=f\"--size 1024 4096 16384 --arg1 64 --iters 1\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 1900 --calc misses_per_iter=L1_cache_misses/iterations\")\n",
    "show_trace(\"./stride_0.hdf5\", show_tag=[\"init0\",\"init1\",\"init2\"], layer_preset=default_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "\n",
    "Here's a screen cap if Moneta is too slow.\n",
    "    \n",
    "![image.png](img/Q4.png)\n",
    "    \n",
    "Everything is purple!  Use the measurement \n",
    "    to see how many individual cachelines are accessed when `size = 16384` (the right-most block of accesses).  You should find that it can now fit comfortably in the cache.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 2,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-2\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Why did increasing the stride size reduce the cache miss rate even though `size` remained the same?</div>\n",
    "\n",
    "<div class=\"answer\">\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Working Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "In lecture you heard about the \"working set\" of an application, and the notion of a working set is deeply tied to temporal locality.\n",
    "The working set is the _portion of memory that the program is currently using_.  The connection between working sets and temporal locality lies in the word \"currently\" since that refers to a period of time.  In essence, the working set is the set of cache lines that a program accesses repeatedly over a period of time.\n",
    "\n",
    "One thing to note:  Without reuse, there can be no temporal locality.  A single access to a cache line has no temporal locality.\n",
    "\n",
    "Generally speaking, there will be fewer cache misses (and performance will be faster) if the working set fits in the L1 cache (or failing that, in the L2 cache).\n",
    "\n",
    "To illustrate how working set size influences cache behavior, we'll use the `set` container object from the C++ standard template library.  Internally, `set` is implemented as a red-black binary tree.  The code below creates an `std::set` and then fills it with 4096 pseudo-random (and non-repeating) `uint64_t` values using `insert()` and then performs a bunch of queries with `find()`.  The crazy `&(*a), &(*a)+ 1` is just to set the bounds for the Moneta tag so we know where the `set` lives in memory.  Run the cell, and it'll open up the resulting trace in Moneta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## One Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "working = fiddle(\"working.cpp\", function=\"working\", opt=\"-O1\",\n",
    "code=r\"\"\"\n",
    "#include\"pin_tags.h\"\n",
    "#include\"function_map.hpp\"\n",
    "#include\"archlab.hpp\"\n",
    "#include<set>\n",
    "#include<cstdint>\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* working(uint64_t * data, uint64_t size, uint64_t arg1) {\n",
    "    auto s = new std::set<uint64_t>();\n",
    "    uint64_t seed = 1;\n",
    "\n",
    "    TAG_START(\"build\", (void*)-1, 0, true);\n",
    "    for(uint x = 0; x < size; x++) {\n",
    "        auto t = fast_rand(&seed);\n",
    "        s->insert(t);\n",
    "        auto a = s->find(t);\n",
    "        TAG_GROW(\"build\",  &(*a), &(*a)+ 1);\n",
    "    }\n",
    "    TAG_STOP(\"build\");\n",
    "    \n",
    "    seed = 1;\n",
    "    \n",
    "    TAG_START(\"search\", (void*)-1, 0, true);\n",
    "    for(uint x = 0; x < size; x++) {\n",
    "        auto a = s->find(fast_rand(&seed));\n",
    "        TAG_GROW(\"search\", &(*a), &(*a)+ 1);\n",
    "    }\n",
    "    TAG_STOP(\"search\");\n",
    "\n",
    "    TAG_START_ALL(\"delete\", false);\n",
    "    delete s;\n",
    "    TAG_STOP(\"delete\");\n",
    "    return data;\n",
    "}\n",
    "\n",
    "FUNCTION(one_array_1arg, working);\n",
    "\"\"\",\n",
    "           run=[\"moneta\"], name=\"set\",\n",
    "           cmdline=f\"--size {4* 1024}   --iters 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "show_trace(\"./set_0.hdf5\", show_tag=['build','search'], layer_preset=[\"misses-compulsory-all\", \"misses-all\", \"hits-all\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Here's a screen cap in Moneta is flaking on you.\n",
    "\n",
    "![image.png](img/Q6.png)\n",
    "\n",
    "You should see a big speckled, multi-colored triangle.  What you are looking at is the region of the heap that the C++ standard library is allocating to hold the set.  Since, it's a tree-based structure, it's made up of many small objects that get allocated with `new`.  The heap is allocating space starting at a low address and working upward -- hence the diagonal.\n",
    "\n",
    "The color key is the same as above:\n",
    "\n",
    "* Compulsory misses are light blue\n",
    "* hits are purple.\n",
    "* Conflict misses are gold.\n",
    "\n",
    "If necessary, lock the X axis and then use the \"hand\" tool to move the triangle down so it sits on the horizontal axis. Be sure not cut off any of the triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Approximately how many bytes does the `set` occupy?  What's the ratio of bytes occupied to the number of values the `set` contains?  How many bytes would be needed to store the same elements in an array?  What is the light blue line along the top slope of the triangle?\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"answer\"\n",
    "    \n",
    "**Bytes for the `set`**: \n",
    "    \n",
    "**Bytes per element in the `set`:** \n",
    "    \n",
    "**Bytes to store elements:**\n",
    "    \n",
    "**What is the blue line?:**\n",
    "     \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "**Bytes for the `set`**:  You can measure this with the ruler tool or \"eyeball\" it from the y-axis.  It's about 195kB\n",
    "    \n",
    "**Bytes per element in the `set`:**  The code inserts 4096 elements, so that's about `195000/4096 = 47.6`.  Which is really inefficient in terms of space!  If you ever need to store very large sets of integers, you might want to consider a different data structure.\n",
    "    \n",
    "**Bytes to store elements in array?:** Much less than this set is using: `4096 * 8 = 16384`.\n",
    "    \n",
    "**What is the blue line?:** The blue line is compulsory misses, so the cache can't help us there.  The fall on a nice line because each access to newly-allocated memory is a compulsory miss.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "In the code, we created two tag: `build` and `search`.  The graph shows both of them.  Let's see which is which.  Click on the \"Tags\" drop down on the right and you'll get a list of the tags in this trace.  Clicking the check box next to the tag will hide and show the memory accesses for that tag.\n",
    "\n",
    "![Turning tags on and off](img/tags.gif)\n",
    "\n",
    "Use the check boxes to figure out which part of the graph is `build` and which is `search`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "\n",
    "Also, recall that the green line on the vertical axis is the size of the cache that Moneta is modeling.  Use the ruler tool to answer this question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 2,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-2\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "At the beginning of the build portion of the experiment what is miss rate?  When does it start to climb?  Why?\n",
    "\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "**Miss rate at the beginning?:**  \n",
    "    \n",
    "**WHen does it start to rise?:** \n",
    "    \n",
    "**Why does it climb?:**  \n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Where are hits more concentrated on the graph?  What part of the data structure do think they are accessing (think carefully about how `std::set` is implemented and accessed)?\n",
    "\n",
    "</div>    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "**Where are they concentrated?:**  \n",
    "    \n",
    "**What part of the data structure are they accessing?  Be as specific as you can.:** \n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "**Where are hits concentrated?:**  They are generally more hits near the bottom of the graph.  There's also an interesting strip of purple right under the compulsory misses during `build`.\n",
    "    \n",
    "**What part of the data structure are hits accessing?:**  These areas of memory correspond to the part of the tree closest to the root.  They are the \"hottest\" part of the tree because they are accessed very frequently:  Every insertion or query touches the root node, half of the queries touch each of the roots children, 1/4 of the queries touch each of the roots grand-children, etc.  \n",
    "  \n",
    "We know that each node in tree takes about 48 bytes, so the cache can hold about `32k /48 = 682` nodes.  That corresponds to the `log2(682) = 9.4` layers of a perfectly balanced binary tree (Red-black trees are not perfectly balanced, but they are pretty balanced).\n",
    "\n",
    "Each time we query the red black tree, the code traverses about  12 levels of the tree.   However, about 9 of them will be in the cache, something like 75% of accesses should hit in the cache.   If we doubled the size of the tree to 8192 nodes, the depth only increases by one, so the hit rate will go from `9/12 = 75%` to `9/13 = 69%`.\n",
    "    \n",
    "In your earlier classes you learned about how trees are good for performance because have logarithmic complexity for insertion and deletion.  That's great, but their structure also provide a high degree of temporal locality because the nodes near the root are accessed so frequently.  This property is widely exploited in many different data storage systems.  For instance, databases typically store very large indexes in a tree with nodes near the root in memory (where they will be accessed quickly and frequently) and the leaves on disk (where access is slow but infrequent).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "If you zoom out in the Moneta trace above, you can find the memory operations in the `delete` tag.  Then there is another column of memory accesses that touches the same area.  What is going on there?\n",
    "    </div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "The STL provides another data structure: `unordered_set` that uses a hash table instead of binary tree.  Copy the code cells above, repeat this whole section with `unordered_set` instead of `set`.  See what you find. \n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# The Three C's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Recall from lecture (or review the slides) that we can classify cache misses into types (known as \"The Thee C's\"):\n",
    "\n",
    "1.  **Compulsory**: These misses occur because the processor has not accessed this cache line before.\n",
    "\n",
    "2.  **Capacity**: These occur because the program is accessing more memory than the cache can hold (i.e., it's working set is bigger than the cache).\n",
    "\n",
    "3.  **Conflict**: These occur because a given cache line of memory can only live in one of the associative sets of the cache.\n",
    "\n",
    "## Capacity and Compulsory Misses\n",
    "\n",
    "The our investigation of spatial locality, temporal locality, and working sets illustrated compulsory and capacity misses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Look back over the Moneta graphs in previous sections of this lab and grab screen captures of examples of compulsory and capacity misses.  Paste them below and describe why the illustrate each kind of miss.\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "**Compulsory**\n",
    "    \n",
    "**Capacity**\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "**Compulsory** The misses that occur in the first time through the inner loop of `stride()` are all compulsory misses.\n",
    "\n",
    "![image.png](img/compulsory.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "\n",
    "**Capacity** In the same experiment, we saw that when the number of cachelines accessed exceeds the number of cache lines in the cache, we get many misses.  These are all capacity misses:\n",
    "    \n",
    "![image.png](img/capacity.png)\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Conflict Misses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's try to produce some conflict misses.  In the last lab, we used a miss machine to generate lots of misses.  They were mostly capacity misses (i.e., we accessed too many cache lines), and the miss machine let us produce lots of seemingly random accesses really fast.  For conflict misses, we need something different:  Highly-organized misses placed precisely.\n",
    "\n",
    "The necessary ingredients for lots of conflicts misses are many memory accesses that will map to the same associative set in the cache.  If we access many of these cache lines, the associative set will \"overflow\" and that will causes misses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Assume our 32kB cache with 64-byte lines and 8-way associativity and 64-bit addresses.  Given an address $A$, how can we compute a new address, $B$, that will map to the same associative set but is not part of the same cache line as $A$?  Given an index, $i$, into an array, how can we compute the index of another element, $j$, that will conflict with the first?\n",
    "\n",
    "</div>\n",
    "<div class=\"answer\">\n",
    "\n",
    "**How do you compute `B`?**\n",
    "\n",
    "**How do you compute `j`?**\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "From our earlier discussion, we know we have:\n",
    "    \n",
    "1.  `log2(64) = 6` offset bits.\n",
    "2.  `32kB/64B = 512` cache lines.\n",
    "3.  `512/8 = 64` associative sets.\n",
    "4.  `log2(64) = 6` index bits.\n",
    "5.  `64-6-6 = 52` tag bits.\n",
    "\n",
    "We also know that two addresses are in the same cache line if their `tag` and `index` bits match and that they map to the same associative set if they have the same `index`.  So, we need $B$ to have the same `index` but a different tag.  The first tag bit is in position `offset bits + index bits = 12`, so adding $2^{12} = 4096$  to $A$ will give $B$ a different `tag` and an identical `index`, which is just what we needed!\n",
    "\n",
    "Computing the index of a conflicting element is easy:  We just divide $2^{12}$ by the size of the elements of the array.  For `uint32_t`, we get $2^{12}/4 = 1024$, so $j = i + 1024$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's see if your formula worked.  We'll run `stride()` from early with a stride of 16 and 1024.  In the experiment below we set `--size` so that we cover 64 strides worth of the memory, since both strides are larger than cache line, each execution of the loop will touch 64 cache lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from notebook import *\n",
    "\n",
    "t = fiddle(\"conflict.cpp\", code=\"\"\"\n",
    "#include\"pin_tags.h\"\n",
    "#include\"CNN/tensor_t.hpp\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* conflict(uint64_t * data, uint64_t size, uint64_t arg1) {\n",
    "    tensor_t<uint32_t> t(size,1,1,1, (uint32_t *)data);\n",
    "    TAG_START(\"init\", t.data, &t.as_vector(t.element_count()), true);\n",
    "\n",
    "        for(uint x = 0; x < size; x+=arg1) {\n",
    "            t.get(x,0,0,0) = x;\n",
    "        }\n",
    "    TAG_STOP(\"init\");\n",
    "    return data;\n",
    "}\n",
    "\n",
    "FUNCTION(one_array_1arg, conflict);\n",
    "\"\"\")\n",
    "\n",
    "stride16 = fiddle(\"conflict.cpp\",\n",
    "           function=\"conflict\", opt=\"-O1\",\n",
    "           run=[\"perf_count\"], name=\"stride16\",\n",
    "           cmdline=f\"--size {16*64} --arg1 16  --iters 10000\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "stride1024 = fiddle(\"conflict.cpp\",\n",
    "           function=\"conflict\", opt=\"-O1\",\n",
    "           run=[\"perf_count\"], name=\"stride1024\",\n",
    "           cmdline=f\"--size {1024*64} --arg1 1024 --iters 10000\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "\n",
    "df = render_csv([\"stride16.csv\", \"stride1024.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "plotPEBar(df=df, what=[(\"arg1\", \"L1_MPI\"),(\"arg1\", \"L1_cache_misses\"), (\"arg1\", \"CPI\"), (\"arg1\", \"ET\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Based on our analysis above, what do you think will happen with if the stride is one cache line longer (1040 bytes) or or one cache line shorter (1008 bytes)?  Why?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "**Stride 1008:**\n",
    "    \n",
    "**Stride 1040:**\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "In the analysis analysis above, we chose 1024 because it was $2^{12}/\\mathrm{sizeof(uint32\\_t)}$, and we choose $2^{12}$ because that stride size would change the tag bits without changing the index bits.  And this, in turn, would cause all the cache lines to fall in a singe associative set.  A look at the binary representation of $2^{12}$ shows with this is true: `1 0000 0000 0000b`. (the `b` means binary).  Since there are no 1's in low order bits, adding $2^{12}$ won't change the index or offset.\n",
    "    \n",
    "For stride 1040 in an array of `uint32_t`, the number of bytes in stride is 4160:  Let's look at 4160 in binary: `1 0000 0100 0000b`.  It has 1 down there, so adding it _will_ change the index.  So, the cache lines _will not_ fall into a single associative set.  Hence, cache misses should much lower.\n",
    "   \n",
    "Let's see what happens:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "stride1008 = fiddle(\"conflict.cpp\",\n",
    "           function=\"conflict\", opt=\"-O1\",\n",
    "           run=[\"perf_count\"], name=\"stride1008\",\n",
    "           cmdline=f\"--size {1023*64} --arg1 1008  --iters 10000\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "stride1040 = fiddle(\"conflict.cpp\",\n",
    "           function=\"conflict\", opt=\"-O1\",\n",
    "           run=[\"perf_count\"], name=\"stride1040\",\n",
    "           cmdline=f\"--size {1025*64} --arg1 1040 --iters 10000\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "\n",
    "df = render_csv([\"stride16.csv\", \"stride1008.csv\", \"stride1024.csv\", \"stride1040.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "plotPEBar(df=df, what=[(\"arg1\", \"L1_MPI\"), (\"arg1\", \"CPI\"), (\"arg1\", \"ET\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "    \n",
    "The number of misses per instruction is almost identical to the stride-16 case!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The main lesson here is that conflict misses are largely product of bad luck:  It may happen that for a particular cache capacity, associativity, and line size, that many cache lines in the application's working set happen to map to the same associative set.\n",
    "\n",
    "Fortunately, in modern processors caches are pretty highly-associative (our is 8-way) and at that level of associativity conflict misses are not a huge problem.  If you working set is smaller than your cache's capacity, you'd have to be very unlucky to have enough cache lines land in the same associative set to cause many conflict misses.  As the example above shows, however, it is not hard to construct programs that are this unlucky.  We have a term for these access patterns:  We say they are \"pathological\".\n",
    "\n",
    "By definition, pathological access patterns are rare, so we don't spend too much time worrying about them.  But they can crop up and it's a good idea to be aware of the possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Consider the implementation of `tensor_t` described earlier in the lab.  Accessing a tensor column-wise produces strided accesses which could lead to conflict misses if the dimensions of the tensor are \"unlucky\".  Why is this so?  What constitutes \"unlucky\" dimensions?  How could you modify `tensor_t` make it (mostly) immune to \"unlucky\" dimensions?\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"answer\">\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# The L2 and L3 Caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "So far in these two labs, we have focused on the L1 cache, but our machine also has L2 and L3 caches.  Here's how they are organized:\n",
    "\n",
    "![image.png](img/cacheorg.png)\n",
    "\n",
    "As a reminder, the L1 is 32kB, 8-way set associative, with 64-byte lines.  So, there are 512 cache lines divided into 16 associative sets.\n",
    "\n",
    "The L1 and L2 are private to each core while the L3 is shared among all the cores on the CPU.  We may look at the L3 in more details when we study multi-core.  For now, we will take a look at the L2.  The L2 is 256kB and is 8-way set associative.\n",
    "\n",
    "The code below is similar to the `stride` function we used in the prevous lab.  The change is that the outer loop is setup so we do the same number of memory accesses for all values of `size` (This is why we divide by `size`).  Our goal is to measure the L1 and L2 MPI as size increases.  The CPU's performance counters don't let us collect L1 and L2 statistics at the same time, so we have to run the experiment once for each cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "As `size` increases, the miss rate for the L1 and L2 will rise.  At value of `size` would you expect to see significant increases in L1 and L2 MPI?\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "**L1 critical `size`:**\n",
    "\n",
    "**L2 critical `size`:**\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "space = lambda x: \" \".join(map(str, x))\n",
    "L2 = fiddle(\"L23.cpp\", function=\"L23\", name=\"L2\", run=[\"perf_count\"], tagged_only=True, opt=\"-O1\",\n",
    "            code=r\"\"\"\n",
    "#include\"pin_tags.h\"\n",
    "#include\"CNN/tensor_t.hpp\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* L23(uint64_t * data, uint64_t size, uint64_t arg1) {\n",
    "    tensor_t<uint32_t> t(size,1,1,1, (uint32_t *)data);\n",
    "    TAG_START(\"init\", t.data, &t.as_vector(t.element_count()), true);\n",
    "\n",
    "    for(uint i = 0; i < (1 << 20)/size; i++) {\n",
    "        for(uint x = 0; x < size; x+=arg1) {\n",
    "            t.get(x,0,0,0) = x;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    TAG_STOP(\"init\");\n",
    "    return data;\n",
    "}\n",
    "\n",
    "FUNCTION(one_array_1arg, L23);\n",
    "\"\"\",\n",
    "           cmdline=f\"--size {space([2**i for i in range(4, 20)])} --arg1 16 --iters 10000\", \n",
    "           perf_cmdline=\"--stat-set L2.cfg --MHz 3500\")\n",
    "\n",
    "L1 = fiddle(\"L23.cpp\", function=\"L23\", name=\"L1\", run=[\"perf_count\"], tagged_only=True, opt=\"-O1\",\n",
    "           cmdline=f\"--size {space([2**i for i in range(4, 20)])} --arg1 16 --iters 10000\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = render_csv(\"L1.csv\")\n",
    "L2_data = render_csv(\"L2.csv\")\n",
    "data[[\"L2_MPI\", \"L2_cache_misses\"]] = L2_data[[\"L2_MPI\", \"L2_cache_misses\"]]\n",
    "display(data[[\"function\",\"size\", \"arg1\", \"IC\", \"CPI\", \"L1_MPI\",\"L1_cache_misses\", \"L2_MPI\",\"L2_cache_misses\"]])\n",
    "plotPE(df=data, logx=2, logy=10, combined=True, lines=True, what=[ (\"size\", 'L2_MPI'), (\"size\", 'L1_MPI')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Do the data match your prediction?   If not, how did it differ?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "**L1 prediction correct?:**\n",
    "\n",
    "**L2 prediction correct?:**\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "As always, we are more interested in performance than MPI.  Let's see how CPI behaved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cse142L.is_response": true,
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plotPE(df=data, logx=2, combined=True, lines=True, what=[ ('size', 'CPI')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 2,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-2\">\n",
    "    \n",
    "#### Based on this data, how much speedup could you expect from reducing your working set size (in bytes) from...\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "**2MB to 1MB?:** \n",
    "\n",
    "**512kB to 128kB?:**  \n",
    "    \n",
    "**512kB to 32kB?:** \n",
    "\n",
    "**32kB to 8kB?:** \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# The TLB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The three levels of on-chip caches set the number of _cache lines_ the processor can quickly access.  As you heard in 142, though, there is another kind of cache in the processor:  the TLB.   Instead of data, the TLB caches the translations from virtual addresses to physical addresses, and its size sets the number of _pages_ your program can access quickly.\n",
    "\n",
    "Here's what our processor has:\n",
    "\n",
    "1. 64 entries for 4kB pages (256kB total)\n",
    "2. An L2 TLB with 1024 entries (8-way set associative; 4MBs total @ 4kB pages).\n",
    "3. 32 entries for 2MB pages (64MB total).\n",
    "4. 4 entries for 1GB pages (4GB total).\n",
    "\n",
    "This is a little more complicated than what you heard about in 142.  First off, there is an L1 TLB _and_ an L2 TLB.  If we think of the L1 TLB as cache for memory translations, then the L2 TLB is exactly analogous to the L2 cache:  If the processor has a TLB miss in the L1 TLB, it can look in the L2 TLB.  One important point:  memory address translation _always_ happens at the L1 cache because _all_ the caches are physically tagged.  This means that the L2 TLB _has nothing to do with the L2 Cache_.  \n",
    "\n",
    "The L2 TLB can cover 4MBs worth of 4kB pages of virtual address space.  If you are using more pages than that, you'll get TLB misses and your performance will suffer. \n",
    "\n",
    "Here's a fun idea!:  Let's use a miss machine to measure the L1 TLB miss latency.\n",
    "\n",
    "The the code below is version of our miss machine code from the last lab but with a few changes:\n",
    "\n",
    "1.  It has a template-configurable link size (`BYTES`).\n",
    "2.  We allocate the `MM` links in array that 4096-byte aligned.\n",
    "3.  We use [`madvise()`](https://man7.org/linux/man-pages/man2/madvise.2.html) to prevent us from using 2MB pages, which Linux will automatically use when it can.  We'll come back to that.\n",
    "4.  We can set the _total size_ of the miss machine _in bytes_ with the `size` parameter.  It should be a multiple of `BYTES`.\n",
    "\n",
    "Read through the code to make sure it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"TLB.cpp\", show=(\"//START\", \"//END\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "There are two parameters we need to set:  The size of `MM` (`BYTES` in the code above) and the `size`.\n",
    " \n",
    "Here's what the `miss()` function looks like for `BYTES = 4096`.  It should be familiar from Lab 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make build/TLB.so\n",
    "do_cfg(\"build/TLB.so\", symbol=\"sym.MM_4096ul__miss_MM_4096ul____MM_4096ul___unsigned_long_\", output=\"tlb.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Using the code above, what values of `BYTES` and `size` should we run the miss machine with to measure the L1-TLB miss latency?  (The fact that there are two experiments listed is a hint that you'll need to run two different experiments.)\n",
    "    \n",
    "</div>\n",
    "   \n",
    "<div class=\"answer\">\n",
    "\n",
    "|| `BYTES` | `size` |\n",
    "|--|------------|--|\n",
    "|Experiment 1| | |\n",
    "|Experiment 2| | |\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "We are trying to measure the L1 TLB miss latency.  What is that exactly?  It's the difference in latency between an L1 cache hit that hits in the TLB and an L1 cache hit that misses in the TLB.  We want the accesses to be cache _hits_ because otherwise, our measurement might include the L1 cache miss latency as well.\n",
    "    \n",
    "So, we need to take two measurements.  For the first, we want to ensure that there are no TLB misses and no L1 cache misses.  For the second measurement, we would like there to be many TLB misses and no L1 cache misses.\n",
    "    \n",
    "**No L1-TLB misses/No L1 cache misses:**  Pretty easy.  We could make `MM` occupy 8 bytes and set `size` to 8B.   This would create a miss machine with a single link, which would result in no misses of any kind.  We could also set `size = 4096` (or actually any thing less than 32kB) and the effect would be the same.\n",
    "    \n",
    "**Many L1 TLB misses/No L1 cache misses** This is trickier:  We need to spread our accesses across many more than 64 4kB pages (to have lots of TLB misses) but we need to access no more than 512 cache lines (so they will all fit in the L1 cache).  Fortunately, 512 is much larger than 64, so we can access one cache line in each of 512 pages.\n",
    "    \n",
    "A good first try would be to set `size` to 512*4kB and `BYTES` to 4096.  That way, each `MM` will occupy one page.  Let's try that!\n",
    "    \n",
    "The code below will invoke `TLB_4096` with `size = 4096` (1 page) and `size = 512*4096` (512 pages).  Here's the code for `TLB_4096`:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "render_code(\"TLB.cpp\", show=\"TLB_4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"TLB.cpp\", name=\"TLB1\", function=[\"TLB_4096\"], opt=\"-O3\", run=['perf_count']\n",
    ", cmdline=f\"--size 4096 {512*4096} --arg1 {128*1024*1024} --iters 1\", \n",
    "     perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "\n",
    "\n",
    "df = render_csv(\"TLB1.csv\", columns=[\"function\",\"size\",\"arg1\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\", \"L1_cache_misses\", \"cycles\"])\n",
    "df[\"load_latency_cyc\"] = df[\"cycles\"]/df[\"arg1\"]\n",
    "df[\"load_latency_ns\"] = df[\"load_latency_cyc\"]*df['CT']*1e9\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "    \n",
    "In this data, changing `size` from 4096 to 512*4096 (2,097,152) increased CPI from 1 to 12. And load latency seems to have increasdeb by 12x to 48 cycles. But let's check our data more closely.  Did the experiments a achieve our goals?\n",
    "\n",
    "The first line looks pretty good. Last lab we saw that L1 hit latency was 4 cycles, which is what we see in the last column.   Also, note that L1 cache misses are very low (i.e., `L1_MPI` is very low).\n",
    "    \n",
    "The second line, however, was supposed to have low L1 hits _and_ high TLB misses.  The `L1_MPI` is quite high: 0.25. Since there are only 4 instructions in the `miss()` loop, that means all the loads are missing in the L1.  But wait, we only have 512 links in our miss machine -- they should all fit in our cache at once!  What's going on?!?!?  Think carefully about what's going on before you read the next paragraph.\n",
    "\n",
    "The problem is conflict misses:  since all the `MM` links are 4kB aligned, they fall into a small number of associative sets in the L1.  Hence, cache misses!  \n",
    "\n",
    "To fix this, we can increase the size of `MM` by one cache line to 4160 bytes.  That will ensure that the `MM` structs are spread across all the associative sets (If this doesn't make sense, working out the cache indexes of `MM` structs of size 4096 vs 4160 should help clear it up.).  Here's the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"TLB.cpp\", name=\"TLB2\", function=[\"TLB_4160\"], opt=\"-O3\", run=['perf_count']\n",
    ", cmdline=f\"--size 4160 {512*4160} --arg1 {128*1024*1024} --iters 1\", \n",
    "     perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = render_csv(\"TLB2.csv\", columns=[\"function\",\"size\",\"arg1\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\", \"L1_cache_misses\", \"cycles\"])\n",
    "df[\"load_latency_cyc\"] = df[\"cycles\"]/df[\"arg1\"]\n",
    "df[\"load_latency_ns\"] = df[\"load_latency_cyc\"]*df['CT']*1e9\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "    \n",
    "This looks better.  The first line is essentially unchanged.\n",
    "\n",
    "The second line is much improved: `L1_MPI` is very low.\n",
    "\n",
    "The new load latency with the L1 TLB misses is 13 cycles -- 9 more than the baseline L1 latency, so an L1 TLB misses takes 9 cycles or about 2.6 ns.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "The measurement above is for a miss to the L1 TLB.  Perform a different experiment to measure the L2 TLB miss latency.  This is harder than it appears at first.\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "The measurements above are based on 4kB pages, but we can also use 2MB \"huge pages\".  Repeat the experiment above to determine whether 2MB TLB entries can also reside in the L2 TLB.      \n",
    "</div>\n",
    " \n",
    "<div class=\"answer\">\n",
    "\n",
    "A few notes:\n",
    "    \n",
    "1.  This one is a little involved.  You'll need to significantly tweak the experiments we did above.\n",
    "2.  Whether 2MB TLB entries can be in the in the L2 TLB is not clearly specified in any documents I have found, so I don't know the answer.\n",
    "3.  To get the system to use 2MB huge pages, remove the call to `madvise()` in `TLB.cpp` and ask `posix_memalign()` to give 2MB-aligned memory.\n",
    "4.  Look in `TLB.cpp` for examples of how to change `BYTES`.  `TLB_2M()` is a good starting point.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Optimizing For Locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Minimizing cache misses is critical for maximizing performance because, as you have seen, even a small number of misses can inflate `CPI` and `ET`.  As a result, programmers who are concerned about performance often spend a lot of effort optimizing there code to reduce misses.\n",
    "\n",
    "Below, we'll take a look a two common optimizations:  Loop reordering and tiling.  \n",
    "\n",
    "In the compiler lab, you explored several other optimizations that compilers apply very effectively.  While there are compilers that apply these (and other) locality optimizations, many do not and even when they do, these locality optimizations do not work as effectively when applied automatically, so performance-obsessed programmers often apply locality optimizations by hand (but, of course, only when profiling and Amdahl's law demonstrates it's potentially profitable!).\n",
    "\n",
    "## Loop Renesting\n",
    "\n",
    "Loop reordering or \"re-nesting\" is an optimization that changes the order in which loops are nested to improve locality.  For instance, consider the code below.  It initializes a 2D tensor, but it does it twice:  The first time, the loop for `x` is on the outside of the loop nest.  The second time, `x` is on the inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    " x_outside = fiddle(\"renest.cpp\", function=\"x_outside\", name=\"x_outside\", run=[\"moneta\",\"perf_count\"], tagged_only=True, opt=\"-O1\",\n",
    "                code=r\"\"\"\n",
    "#include\"pin_tags.h\"\n",
    "#include\"CNN/tensor_t.hpp\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* x_inside(uint64_t * data, uint64_t size, uint64_t arg1) {\n",
    "    tensor_t<uint32_t> t(size/arg1,arg1,1,1, (uint32_t *)data);\n",
    "    \n",
    "    TAG_START(\"x_inside\", t.data, &t.as_vector(t.element_count()), true);\n",
    "\n",
    "    for(uint y = 0; y < arg1; y++) {\n",
    "        for(uint x = 0; x < size/arg1; x++) {\n",
    "            t.get(x,y,0,0) = x;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    TAG_STOP(\"x_inside\");\n",
    "\n",
    "    return data;\n",
    "}\n",
    "\n",
    "FUNCTION(one_array_1arg, x_inside);\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* x_outside(uint64_t * data, uint64_t size, uint64_t arg1) {\n",
    "    tensor_t<uint32_t> t(size/arg1,arg1,1,1, (uint32_t *)data);\n",
    "    \n",
    "    TAG_START(\"x_outside\", t.data, &t.as_vector(t.element_count()), true);\n",
    "\n",
    "    for(uint x = 0; x < size/arg1; x++) {\n",
    "        for(uint y = 0; y < arg1; y++) {\n",
    "            t.get(x,y,0,0) = x;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    TAG_STOP(\"x_outside\");\n",
    "    \n",
    "    return data;\n",
    "}\n",
    "\n",
    "FUNCTION(one_array_1arg, x_outside);\n",
    "\"\"\",\n",
    "            cmdline=f\"--size {1024*1024} --arg1 {1024*4}\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "\n",
    "x_inside = fiddle(\"renest.cpp\", function=\"x_inside\", name=\"x_inside\", run=[\"moneta\",\"perf_count\"], tagged_only=True, opt=\"-O1\",\n",
    "\n",
    "                  cmdline=f\"--size {1024*1024} --arg1 {1024*4}\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "show_trace(\"./x_inside_0\", show_tag=[\"x_inside\"], layer_preset=[\"x_inside\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "show_trace(\"./x_outside_0\", show_tag=[\"x_outside\"], layer_preset=[\"x_outside\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Amazingly, those two plots contain exactly the same number of memory accesses, they just distributed differently through time.\n",
    "\n",
    "Recall from our earlier discussion of `tensor_t`, that incrementing the first argument to `get()` corresponds to moving to the next element in the underlying array of data.  In the code above, `x` is the first argument to `get()`, so putting the `x` loop inside leads to better spatial locality.\n",
    "\n",
    "You can see this reflected in the traces:  With `x` on the inside, the program marches linearly through memory.   With the `x` loop outside, it takes large strides through the array.  In particular, it doesn't access the same 64 byte cache line again until long after it has been evicted from the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "   \n",
    "<div class=\"question-text\">\n",
    "\n",
    "Use Moneta's measurement tool to measure the cache miss rate for both versions of the code.\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "| version | hit rate | \n",
    "|---------|----------|\n",
    "| x loop inside           | | \n",
    "| x loop outside           | | \n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "   \n",
    "<div class=\"question-text\">\n",
    "\n",
    "What value of `--arg1` should result in a very high (e.g., > 95%) hit rate, even with the `x` loop on the outside?  Try to reason through the correct value before running any experiments.\n",
    "\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "Recall how `tensor_t` accesses it's internal array.  For the function above, the index calculation becomes:\n",
    "    \n",
    "```\n",
    "index = y * size/arg1 + x;\n",
    "```\n",
    "    \n",
    "When the `x` loop is on the outside, the first iteration of the `y` loop code brings in a cache line and uses one element.  The next iteration of the `y` loop accesses an element that is `size/arg1 ` elements away.  As long as `size/arg1 ` represents more than 64 bytes, that location will be in a different cache line.  In the code above, each element is a `uint32_t`, so if `size/arg1  > 16`, the next element is in a new cache line. \n",
    "    \n",
    "So, each iteration of the `y` loop touches a different cache line.  In our code, that equates to `arg1` cache lines.\n",
    "    \n",
    "When the `y` loop is finished, `x` will increment and the `y` loop will access access the same cache lines in the same order.\n",
    "    \n",
    "The misses arise when `arg1` is larger than the number cache lines in our cache.  Our cache is 32kB with 64-bytes lines, so it has 512 cache lines.  As a result, if `arg1 > 512` there will be lots of cache misses.  You can verify this by running the code above with `arg1` equal to 511, 512, and 513.\n",
    "\n",
    "Remember, though, that Moneta provides a fully-associative cache model.  The real cache is 8-way associative.  The cell below runs `x_outside` for a wide range of `arg1` values.  The cell below graphs the resulting misses per instructions vs. `arg1`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "\n",
    "def srange(low, high, step):\n",
    "    return \" \".join(map(str,range(low, high, step)))\n",
    "\n",
    "tuned_size = fiddle(\"renest.cpp\", function=\"x_outside\", name=\"tuned_size\", run=[\"perf_count\"], opt=\"-O1\", \n",
    "                    cmdline=f\"--size {128*16*1024} --arg1 {srange(512-256, 512+256, 4)} --stat-set L1.cfg --iters 8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "\n",
    "ts1 = render_csv(\"tuned_size.csv\")\n",
    "plotPE(df=ts1, what=[(\"arg1\",\"L1_MPI\")], lines=True, average_by=\"arg1\" ,columns=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "    \n",
    "It's not very clean, but the increase occurs around the right place.  The two most notable things are the spikes in `L1_MPI` below 512 and the relatively slow increase in MPI past 512.\n",
    "\n",
    "The spikes are due to the real L1 cache's limited associativity.  The L1 has 512 cache lines with 8 lines per associative set, so there are 64 sets.  For certain unlucky values of `arg1`, many of the cache lines that the loop accesses will land in the same associative set.\n",
    "    \n",
    "The slow rise in MPI after 512 is a little harder to explain, but there are two likely candidates:\n",
    "    \n",
    "1. Intel's cache replacement policies are reported to guard against poorly behave loops (which `x_outside` is) \"blowing out\" the cache.\n",
    "2. Intel's processors also include several prefetchers that predict accesses with a constant stride, which `x_outside` certainly has.  I've disabled as many of the prefetchers as I have found documentation for, but graphs like this make me think there's still one running somewhere.  \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Loop Tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Renesting loops can improve spatial locality, but it is generally less effective for improving temporal locality.  There are two criteria that must be met in order to exploit temporal locality:\n",
    "\n",
    "1.  The cache line must be re-used.\n",
    "2.  The re-use must occur before the cache line is evicted by other cache lines coming in the cache.\n",
    "\n",
    "This second condition has a direct connection to working set size:  If the working set size of a piece of code is too large, it is likely that parts of it will be evicted before they are accessed again, making it hard for the processor to exploit the temporal _and_ spatial locality.\n",
    "\n",
    "Our goal, then is to shrink the working set so that it fits in the cache and we can exploit the resulting locality.\n",
    "\n",
    "As an example, let's consider a 1-dimensional convolution.  \n",
    "\n",
    "### 1-D Convolution\n",
    "\n",
    "One-dimensional convolution a simple algorithm and a fundamental building block for many signal processing systems.   The inputs are two 1-dimensional arrays (we will use `tensor_t<uint32_t>`) that we will call the `source` and the `kernel` and it produces a third array called the `target`.  The `kernel` is much smaller than the `source` and the `length(target) = length(source) - length(kernel)`.\n",
    "\n",
    "Conceptually, we compute the entries of `target` by \"sliding\" `kernel` along `source` and computing the dot product at each position.  Here's a video that illustrates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "display(IFrame(\"https://www.youtube.com/embed/ulKbLD6BRJA\", width=560, height=315))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code is below.   Run the cell to generate a Moneta trace for a 16kB source and 4kB kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"convolution.cpp\", show=(\"//START\", \"//END\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run the cell below to take a look at the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "fiddle(\"convolution.cpp\", function=\"convolution\", analyze=False, \n",
    "       name=\"convolution\", run=[\"moneta\"], tagged_only=False, \n",
    "       opt=\"-O3\", \n",
    "       cmdline=f\"--size {4*1024} --size2 {1024} --size3 {4*1024}  --tile-size 64\")\n",
    "show_trace(\"./convolution_0\", show_tag=[\"source\", \"kernel\",\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You should see three strips of color.  Yellow line is `target`, the light blue band is `kernel`, and the `source` is in purple.  The slow shift upward of `source` shows the sliding slice of `source` that `kernel` is being dot-producted (dot-produced?) with.\n",
    "\n",
    "Let's see how much of the cache the program is taking up.  Use the measuring tool to select a narrow, vertical band from top to bottom and spans 1M memory accesses. Check the number of cache lines being touched and the cache hit rate.  This is the a rough measure of the working set of the algorithm.  You can drag the measured area around to see how the size of the working set changes (or doesn't).\n",
    "\n",
    "![Oct-01-2021%2022-35-16.gif](img/strips_of_color.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "Based on measurements with the ruler tool, how big (in cache lines) is the working set for this computation?  Does it vary throughout the computation?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "    \n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Memory Behavior in 1-D Convolution\n",
    "\n",
    "The measurement tool can measure the working set of this computation: if we take a narrow, vertical slice of the trace, that tells us how many cache lines the program is using during that period.  If that working set is larger than the cache, than it's likely we will have poor performance.  \n",
    "\n",
    "We can see from the trace that there is quite a bit of reuse:  The code reads `kernel` over and over again, and there's a lot of overlap between the parts of `source` that the program accesses.  There is _a lot_ of temporal locality, and we should be able to use it.\n",
    "\n",
    "Here's the assembly for `do_convolution()`.  A few notes:\n",
    "\n",
    "1. `%rdi` points to `source`\n",
    "2. `%rsi` points to `kernel`\n",
    "3. `%rdx` points to `target`\n",
    "4.  Note how constant propagation and inlining have turned all those calls to `get()` into very simple code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "do_cfg(\"build/convolution.so\", symbol=\"do_convolution\", output=\"convolution1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 2,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-2\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "How many loads does the inner loop perform per iteration?  How many stores? (Recall that `op rl, r2` in x86 means `r2 = r1 op r2`).  If `source` has 4096 element and `kernel` has 512 element, what's the dynamic instruction count for each loop (to within 10%)?  What fraction of the execution is spent on the outer loop?\n",
    "    \n",
    "</div>\n",
    "<div class=\"answer\">\n",
    "\n",
    "|  | Inner loop | Outer loop (excluding the inner loop) | \n",
    "|--|-------------|----------|\n",
    "| Instructions | \n",
    "| Loads | \n",
    "| Stores | \n",
    "\n",
    "    \n",
    "|  | Inner loop | Outer loop (excluding the inner loop) |\n",
    "|--|-------------|----------|\n",
    "| Dynamic Instruction count|  \n",
    "| % of IC | \n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "Compute (rather than measure as you did above) the size of the working set of this computation?  What would it be if `kernel` were 32kB, and `source` was 128kB.  Roughly estimate the cache hit rate in each case.\n",
    "    \n",
    "</div>\n",
    "<div class=\"answer\">\n",
    " \n",
    "**Working set for 16kB source and 4kB kernel**:\n",
    "\n",
    "**Estimated hit rate**:\n",
    "    \n",
    "**Working set for 128kB source and 32kB kernel**:\n",
    "\n",
    "**Estimated hit rate**:\n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "In both cases, working set is roughly twice the size of `kernel`.  Each iteration accesses all of `kernel`, an equal-sized chunk of `source`, and one element of `target`.\n",
    "    \n",
    "For a 4kB kernel (`size = 1024`), this is 8kB (or 128 cache lines).  For a 32kB kernel, it 64kB or about 1024 cache lines.\n",
    "\n",
    "To estimate the cache miss rate, first note that for the smaller data size, the working set is smaller than the L1, so the hit rate should be near zero.\n",
    "    \n",
    "For the larger size, we need to think through the source code.  From the previous question (and Amdahl's Law), we know know that the inner loop is all that really matters, so we can focus there.\n",
    "\n",
    "Each iteration of the inner loop performs 4 memory operations:\n",
    "\n",
    "<div class=\"answer\">\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| first `movl` | load from `source`| \n",
    "| `mull` | load from `kernel`| \n",
    "| `movl %r8d, (%r9) | Store to `target`|\n",
    "| `movl (%rsi) %ecx` | load the size of `kernel` | \n",
    "\n",
    "</div>\n",
    "    \n",
    "The `movl` and the `mull` are both going to take capacity misses for each cache line they access, even though the tend to access the same data from iteration of the outer loop to the next.   Between them they read 64kB of data, too much for the L1 to hold.  Since `uint32_t` is 4 bytes, there are 16 of them per cache line.  Therefore, both of these instructions will miss 1/16 of the time. \n",
    "\n",
    "The store to `target` and the final `movl` will usually be a hit, since `%rsi` and `%r9` only changes in the outer loop. \n",
    "\n",
    "I don't know why the load from `%rsi` is there, but I couldn't get the compiler to get rid of it (exercise for the reader).\n",
    "\n",
    "So, there are likely to be 2 * 1/16 of a cache miss per iteration of the inner loop.  There are 8 instructions in the inner loop, so the `L1_MPI` should be about 0.015.\n",
    "\n",
    "Let's check:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "\n",
    "fiddle(\"convolution.cpp\", function=\"convolution\", analyze=False, name=\"little\", run=[\"perf_count\"], tagged_only=False, opt=\"-O3\",\n",
    "                      cmdline=f\"--size {4*1024} --size2 {1024} --size3 {4*1024} --tile-size 64\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "fiddle(\"convolution.cpp\", function=\"convolution\", analyze=False, name=\"big\", run=[\"perf_count\"], tagged_only=False, opt=\"-O3\",\n",
    "                      cmdline=f\"--size {32*1024} --size2 {8*1024} --size3 {32*1024}  --tile-size 64\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "\n",
    "render_csv(\"big.csv\").append(render_csv('little.csv'))[[\"size\", \"size2\", \"size3\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "    \n",
    "Pretty good agreement!  The `L1_MPI` should match our prediction for the large data size and be roughly 0 for the small data size.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Tiling 1-D Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We are going to \"tile\" the execution of this function to reduce the number of cache misses for large data sets.  Tiling works by breaking up the execution of a set of nested loops into smaller parts with smaller working sets.  If the resulting working set fits in the cache, the number of cache misses should drop significantly.\n",
    "\n",
    "We'll perform the tiling in two steps:\n",
    "\n",
    "1.  We'll \"split\" a loop into two nested loops. \n",
    "2.  Then we'll renest the resulting loops.\n",
    "\n",
    "To split a loop, we will break the loop into fixed-size chunks.   The outer loop will iterate over the chunks, and the inner loop will iterate over the elements within a chunk.  This first transformation _has no effect_ on the order in which computation occurs.\n",
    "\n",
    "Here's version of the convolution code with the `kernel` loop split into chunks of size `tile_size` (we'll set `tile_size` to 64 for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"convolution.cpp\", show=\"do_convolution_new_loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "fiddle(\"convolution.cpp\", function=\"convolution_new_loop\", analyze=False, name=\"convolution_new_loop_small\", run=[\"moneta\"], tagged_only=False, opt=\"-O3\",\n",
    "            cmdline=f\"--size {4*1024} --size2 {1024} --size3 {4*1024}  --tile-size 64\")\n",
    "fiddle(\"convolution.cpp\", function=\"convolution_new_loop\", analyze=False, name=\"convolution_new_loop_big\", run=[\"moneta\"], tagged_only=False, opt=\"-O3\",\n",
    "        cmdline=f\"--size {32*1024} --size2 {8*1024} --size3 {32*1024}  --tile-size 64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Here's the trace on as small data set.  It looks just like it did in the original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "show_trace(\"./convolution_new_loop_small_0\", show_tag=[\"source\", \"kernel\", \"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Here it is the same code on a much larger data set that doesn't fit in the L1.  The slop of the lines is less pronounced because are only seeing the first 10 million memory accesses (which is all that poor datahub's brain can handle).  You might notice that the data structures are laid out differently (i.e., the color stripes are in a different order) in this plot than the one above.  This is the memory allocator putting things wherever it wants.  The color scheme is the same across all these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "show_trace(\"./convolution_new_loop_big_0\", show_tag=[\"source\", \"kernel\", \"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The next step is to renest the split loop.  Here's the code.  The only change is that we swapped the order of the `jj` loop and the `i` loop. Now, we will run the whole algorithm for 2048 elements of the `kernel` at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_code(\"convolution.cpp\", show=\"do_convolution_tiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We'll run it first, with a small data set so you can see what the renesting does.   Here it is on a small data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "fiddle(\"convolution.cpp\", function=\"convolution_tiled\", analyze=False, name=\"convolution_tiled_little\", run=[\"moneta\"], tagged_only=False, opt=\"-O3\",\n",
    "            cmdline=f\"--size {4*1024} --size2 {1024} --size3 {4*1024} --tile-size 64\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "\n",
    "show_trace(\"./convolution_tiled_little_0\", show_tag=[\"source\", \"kernel\", \"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Well, that's different! \n",
    "\n",
    "Let's take a look at the access pattern for each of the tensors:\n",
    "\n",
    "**`kernel`**\n",
    "\n",
    "The kernel is in light blue and labeled with tag 'kernel'.  If you click on \"tags\" on the right, you'll see a list of the tags.  CLick the magnifying glass next to \"kernel\" and you'll zoom into the kernel.\n",
    "\n",
    "The chunking structure shows up as a \"stair step\" pattern.  Each of the light blue blocks shows the portion of the `kernel` tensor the code is processing for each chunk.\n",
    "\n",
    "If you zoom way in, you'll see that the code repeatedly accesses the whole chunk, updating the sum stored in each entry of tensor.\n",
    "\n",
    "Rerun the cell above or press the \"undo\" arrow in the viewer to zoom back out.\n",
    "\n",
    "**`target`**\n",
    "\n",
    "Zoom in on `target`.  Zoom waaay in so you can see individual accesses.  You'll see that for each chunk, we make a linear pass over `target`, accessing each element once.\n",
    "\n",
    "**`source`**\n",
    "\n",
    "Zoom in on `source`.  AT first, it looks like `target`, but if you zoom in further you'll see that the purple lines are \"thick\":  The code accesses each element of the tensor repeatedly during each chunk.\n",
    "\n",
    "`target` and `source` both exhibit reuse -- one of the two criteria necessary to leverage spatial locality.  The second criteria is that the working set fits in the cache.\n",
    "\n",
    "Let's run it on a data set that won't fit in the L1.  The trace shows just 1.5 or so chunks worth of executino.  The \"sawtooth\" pattern would continue if we could trace more accesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "fiddle(\"convolution.cpp\", function=\"convolution_tiled\", analyze=False, name=\"convolution_tiled_big\", run=[\"moneta\"], tagged_only=False, opt=\"-O3\",\n",
    "       cmdline=f\"--size {32*1024} --size2 {8*1024} --size3 {32*1024} --tile-size 64\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "show_trace(\"./convolution_tiled_big_0\", show_tag=[\"source\", \"kernel\", \"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Use the ruler tool to measure the working set size by measuring 1M memory operations. What's the cache hit rate?\n",
    "    \n",
    "</div>\n",
    "<div class=\"answer\">\n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's run the code again with twice as much data.  This time, the trace won't even cover 1 chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "convolution = fiddle(\"convolution.cpp\", function=\"convolution_tiled\", analyze=False, name=\"convolution_tiled\", run=[\"moneta\"], tagged_only=False, opt=\"-O3\",\n",
    "            cmdline=f\"--size {64*1024} --size2 {16*1024} --size3 {64*1024}  --tile-size 64\")\n",
    "\n",
    "\n",
    "show_trace(\"./convolution_tiled_0\", show_tag=[\"source\", \"kernel\", \"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Use the ruler tool to measure the working set size by measuring 1M instructions.  For larger data, what's the cache miss rate?\n",
    "</div>\n",
    "<div class=\"answer\">\n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "So the hit rate doesn't change with the input size and it's very, very high!\n",
    "\n",
    "Let's compare the performance of the original (un-tiled) version with our tiled version on the same data set.  In theory, the tiled version should be much faster because it has better temporal locality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make clean\n",
    "fiddle(\"convolution.cpp\", function=[\"convolution\", \"convolution_tiled\"], analyze=False, name=\"convolution\", run=[\"perf_count\"], tagged_only=False, opt=\"-O3 \",\n",
    "            cmdline=f\"--size {64*1024} --size2 {16*1024} --size3 {64*1024} --tile-size 64\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "#fiddle(\"convolution.cpp\", function=\"convolution_tiled\", analyze=False, name=\"convolution_tiled\",run=[\"perf_count\"], tagged_only=False, o\n",
    "#            cmdline=f\"--size {64*1024} --size2 {16*1024} --size3 {64*1024} --tile-size 64\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_csv(\"convolution.csv\", columns=[\"function\", \"size\", \"size2\", \"size3\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Hmmm....\n",
    "\n",
    "What happened?  L1_MPI is low, CPI went down, but ET went up!\n",
    "\n",
    "Ugh! IC rose by 1.3x!  Crud.  Let's look at the assembly.  On the left is the untiled version.  On the right, it is tiled. (It might help to open the images below in separate tabs or download and print them.  That's what I had to do.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "from notebook import *\n",
    "compare([do_cfg(\"build/convolution.so\", symbol=\"sym.do_convolution\", output=\"normal\"), do_cfg(\"build/convolution.so\", symbol=\"sym.do_convolution_tiled\", output=\"tiled\")], [\"not tiled\", \"tiled\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 6,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-6\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "Find the innermost loop body in both CFGs.  Fill out the table below assuming that `kernel` is 64kB (16k entries) and `source` is 512kB (64k entries). \n",
    "    \n",
    "</div>\n",
    "<div class=\"answer\">\n",
    "\n",
    "| |# of times executed | static insturctions  | dynamic instructions | \n",
    "|--|-|-|-|\n",
    "|untiled | \n",
    "|tiled | \n",
    "\n",
    "**Ratio of Tiled IC/Untiled IC**: \n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "In the tiled code, there are some extra instructions in the inner loop.  They are due to the more complex loop completion condition required by tiling.  To put it another way:  The higher loop overhead increased IC more than better locality reduce CPI.\n",
    "\n",
    "Use this fiddle to answer the question below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "Modify the fiddle below (it's the same code as `convolution_tiled()`) to reduce loop overhead and achieve speedup with tiling.\n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "\n",
    "convolution = fiddle(\"convolution_question.cpp\", function=\"convolution_question\", analyze=False, run=[\"perf_count\"], tagged_only=False, opt=\"-O3\",\n",
    "                code=r\"\"\"\n",
    "#include\"pin_tags.h\"\n",
    "#include\"CNN/tensor_t.hpp\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "\n",
    "extern \"C\"\n",
    "void do_convolution_question(const tensor_t<uint32_t> & source,\n",
    "                    const tensor_t<uint32_t> & kernel,\n",
    "                    tensor_t<uint32_t> & target, int32_t tile_size) {\n",
    "\n",
    "    for(int32_t jj = 0; jj < kernel.size.x; jj += tile_size) {  // Move the jj chunk loop outside\n",
    "        for(int32_t i = 0; i < target.size.x; i++) {\n",
    "            for(int32_t j = jj; j < kernel.size.x && j < jj + tile_size; j++) {\n",
    "                target.get(i,0,0,0) += source.get(i + j,0,0,0) * kernel.get(j,0,0,0);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "  \n",
    "\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* convolution_question(uint64_t * source_space, uint64_t source_size,\n",
    "                      uint64_t * kernel_space, uint64_t kernel_size, \n",
    "                      uint64_t * target_space, uint64_t _target_size,\n",
    "                      int32_t tile_size) {\n",
    "    tensor_t<uint32_t> source(source_size,1,1,1, (uint32_t *)source_space);\n",
    "    tensor_t<uint32_t> kernel(kernel_size,1,1,1, (uint32_t *)kernel_space);\n",
    "    uint64_t target_size = source_size - kernel_size;\n",
    "    tensor_t<uint32_t> target(target_size,1,1,1, (uint32_t *)target_space);\n",
    "    TAG_START(\"source\", source.data, &source.as_vector(source.element_count()), true);\n",
    "    TAG_START(\"kernel\", kernel.data, &kernel.as_vector(kernel.element_count()), true);\n",
    "    TAG_START(\"target\", target.data, &target.as_vector(target.element_count()), true);\n",
    "\n",
    "    // Here's the the key part:\n",
    "    do_convolution_question(source, kernel, target, tile_size);\n",
    "    \n",
    "    TAG_STOP(\"source\");\n",
    "    TAG_STOP(\"kernel\");\n",
    "    TAG_STOP(\"target\");\n",
    "    return target_space;  \n",
    "}\n",
    "\n",
    "FUNCTION(convolution, convolution_question);\n",
    "\n",
    "\"\"\",\n",
    "cmdline=f\"--size {64*1024} --size2 {16*1024} --size3 {64*1024}  --tile-size 64\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "data = render_csv(\"convolution.csv\").append(render_csv(\"build/convolution_question.csv\"))\n",
    "data[[\"function\", \"size\", \"size2\", \"size3\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "Loop overhead is additional instructions that are necessary to implement and terminate the loop:  Exit condition checks, counter increments, branches.  Each iteration incurs these overheads, so the total overhead is equal to the number of iteration multiplied by the per-iteration overhead.  So, we can reduce the total overhead by reducing the number of iterations or the per-iteration overhead.\n",
    "\n",
    "Let's start with reducing the number of iterations.  The easiest way to achieve this would be to unroll the loop.  For some reason the compiler choose not to unroll this loop, but let's force it to:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "render_code(\"convolution.cpp\", show=\"do_convolution_tiled_unrolled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "fiddle(\"convolution.cpp\", function=\"convolution_tiled_unrolled\", analyze=False, name=\"convolution_tiled_unrolled\", run=[\"perf_count\"], tagged_only=False, opt=\"-O3\",\n",
    "            cmdline=f\"--size {64*1024} --size2 {16*1024} --size3 {64*1024}  --tile-size 64\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "data = render_csv(\"convolution.csv\").append(render_csv(\"convolution_tiled_unrolled.csv\"))\n",
    "data[[\"function\", \"size\", \"size2\", \"size3\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "\n",
    "It's a little better, but not much. It appears the compiler's decision not to unroll was not quite correct.\n",
    "\n",
    "But why didn't it help more?  Let's look at the unrolled assembly:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "do_cfg(\"build/convolution.so\", symbol=\"sym.do_convolution_tiled_unrolled\", output=\"convolution2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "\n",
    "What a mess!\n",
    "\n",
    "It appears to be a huge jumble but it's not actually that complicated.  Here's the pseudo code for what's (roughly) going on the unrolled loop:\n",
    "\n",
    "```\n",
    "for(i =0; i < bound; i+=8) {\n",
    "    do_iteration(i++);\n",
    "    if (i <= bound) goto end;\n",
    "    do_iteration(i++);\n",
    "    if (i <= bound) goto end;\n",
    "    do_iteration(i++);\n",
    "    if (i <= bound) goto end;\n",
    "    do_iteration(i++);\n",
    "    if (i <= bound) goto end;\n",
    "    do_iteration(i++);\n",
    "    if (i <= bound) goto end;\n",
    "    do_iteration(i++);\n",
    "    if (i <= bound) goto end;\n",
    "    do_iteration(i++);\n",
    "    if (i <= bound) goto end;\n",
    "    do_iteration(i++);\n",
    "}\n",
    "end:\n",
    "```\n",
    "\n",
    "It really isn't saving any loop bound checks, so it's not surprising that IC doesn't change much.\n",
    "\n",
    "Don't give up yet!  We can do better.\n",
    "\n",
    "We need to make the compiler's job easier.  The reason for all the bounds checks in the unrolled loop is that the bound could be anything.  If we constrain the loop bound, the compiler can unroll more efficiently.\n",
    "    \n",
    "We have not solved the problem, but we can now ask a more focused question.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "Go back up to the fiddle you worked on earlier.  Try to reduce loop overhead due to unrolling. Feel free to start with the code for `convolution_tiled_unrolled()`.\n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "Most of the time, the inner loop runs from `jj` to `jj+tile_size`.  But, once every outer loop iteration, it might run fewer iterations if `jj+tile_size > kernel.size.x`.\n",
    "    \n",
    "As mentioned, in Lab 2, gcc likes to unroll loops 8 times.  This means that if `tile_size` is a multiple of 8, then the common case (`j = jj..jj+tile_size`) should not required any bounds checks in the unrolled loop body.  The compiler can just do this:\n",
    "\n",
    "    \n",
    "```\n",
    "for(j = jj; j < jj + tile_size; i+=8) {\n",
    "    do_iteration(i++);\n",
    "    do_iteration(i++);\n",
    "    do_iteration(i++);\n",
    "    do_iteration(i++);\n",
    "    do_iteration(i++);\n",
    "    do_iteration(i++);\n",
    "    do_iteration(i++);\n",
    "    do_iteration(i++);\n",
    "}\n",
    "```\n",
    "\n",
    "So, how can we 1) ensure that `tile_size % 8 == 0` and 2) separate out the `j = jj..jj+tile_size` case from the occasional boundary condition?  \n",
    "                  \n",
    "For #1, we can just check chop off the 3 low-order bits of `tile_size`.  Note that it's not sufficient to just pass multiples of 8 with `--tile-size` parameter, the compiler must know for certain that `tile_size` is multiple of 8.\n",
    "    \n",
    "For #2, we can check for the boundary condition and handle it as a special case, but do it _outside the loop_.  Then, we'll rely on the compiler to unroll:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "render_code(\"convolution.cpp\", show=\"do_convolution_tiled_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "fiddle(\"convolution.cpp\", function=\"convolution_tiled_split\", analyze=False, name=\"convolution_tiled_split\", run=[\"perf_count\"], tagged_only=False, opt=\"-O3\",\n",
    "            cmdline=f\"--size {64*1024} --size2 {16*1024} --size3 {64*1024}  --tile-size 64\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "data = render_csv(\"convolution.csv\").append(render_csv(\"convolution_tiled_unrolled.csv\")).append(render_csv(\"convolution_tiled_split.csv\"))\n",
    "data[[\"function\", \"size\", \"size2\", \"size3\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "\n",
    "Finally!  AT long last!  Speedup greater than 1! 1.46x in fact!\n",
    "\n",
    "Let's checkout the CFG:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "do_cfg(\"build/convolution.so\", symbol=\"sym.do_convolution_tiled_split\", output=\"convolution3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"_solution\">\n",
    "\n",
    "Check out that huge basic block!  Such efficiency!  Such an absence of loop condition checks!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Discussion\n",
    "\n",
    "Let's take a moment to think about why that was so hard.  Let's address two questions.\n",
    "\n",
    "**First question** Why was it so hard to realize significant performance improvements for 1-D convolution by reducing cache misses?  \n",
    "\n",
    "There are two main reasons:  First, the inner loop of convolution is very small, so the extra loop overhead from tiling really killed us.  If the loop body had been larger, the relative impact of the loop overhead would have been smaller.\n",
    "\n",
    "Second, there was no loop-carried dependence between loads:  None of the loads in one iteration needed to finish before the loads in the next iteration could begin.  This means there was a lot of instruction level parallelism (ILP, which you are just learning about in 142) and, in particular, there is a lot of memory parallelism.  This mean that multiple loads (and probably multiple cache misses) ran in parallel.  When multiple long-latency operation run in parallel, we say their latency is \"hidden\".  We'll talk about that more in Lab 5.\n",
    "\n",
    "**Second question** What lessons should you take away from this example? \n",
    "\n",
    "The most important lesson is about the process:  I made small changes, measured their impact, study the code to understand the cause, and made changes to try to improve things.  This is one part of how you should approach the programming assignment.\n",
    "\n",
    "A secondary lesson is the \"trick\" with the `if` statement to make the common case faster.  You should _not_ just decide to apply that trick to every loop you encounter, but it is a good tool to have.  Like all manual optimizations, though, it should only be applied when you have data to suggest it'll work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "Take what you've learned about loop unrolling and see if you can speedup the `baseline` implementation _without_ tiling.  Can you beat the `tiled-split` version?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "I'm divided on whether the `if` around the two versions of the inner loop is elegant or ugly.  Can you find a more elegant way to express the loop bound that allows the compiler to unroll the loop effectively?  Do you think it makes the performance characteristics of the code more or less maintainable? (\"maintainable\" in this case means that it is unlikely that someone later will inadvertently change the code in a way the causes the compiler to no longer unroll the loop correctly)\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Tuning Tile Size\n",
    "\n",
    "Shockingly, we aren't done with 1D convolution yet!  What about the `tile_size`?  What's the right value?  \n",
    "\n",
    "We could try to do some math to figure out exactly how big it should be, but at this point, you're tired and I so am I.  Let's just check experimentally.  We'll run it with powers of 2 from 8 to 8k and see what's best.  (We can't do 1, 2, or 4 because they are not multiples of 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "fiddle(\"convolution.cpp\", function=\"convolution_tiled_split\", analyze=False, name=\"tile_size\", run=[\"perf_count\"], tagged_only=False, opt=\"-O3\",\n",
    "            cmdline=f\"--size {64*1024} --size2 {16*1024} --size3 {64*1024}  --tile-size 8 16 32 64 128 256 1024 2048 4096 8192\", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(render_csv(\"tile_size.csv\", columns=[\"tile_size\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]))\n",
    "data = render_csv(\"convolution.csv\").append(render_csv(\"convolution_tiled_unrolled.csv\"))\n",
    "data = data.append(render_csv(\"convolution_tiled_split.csv\")).append(render_csv(\"tile_size.csv\").iloc[6])\n",
    "plotPE(\"tile_size.csv\", what=[(\"tile_size\", \"IC\"),(\"tile_size\", \"CPI\"),(\"tile_size\", \"ET\"), (\"tile_size\", \"L1_MPI\")], logx=2, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We want to minimize `ET`.  There's pretty broad minimum area where the `IC` (why does increasing `tile_size` reduce IC?) is pretty low and `CPI` is not too high due to `L1_MPI` shooting up when `tile_size` gets big enough to blow out the L1 cache.\n",
    "\n",
    "Here's all our data so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(data[[\"function\", \"tile_size\",\"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Turing `tile_size` gave us an additional 0.32/0.27 = 1.14x.  This is mostly from reducing `IC`.  Interestingly, we also dropped `L1_MPI` by a factor of 10, although it was already very low, so the impact is negligible.\n",
    "\n",
    "There's one more things we'll try before call it a day:  We can help the compiler a bit more by converting `tile_size` to a constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"convolution.cpp\", show=\"do_convolution_tiled_fixed_tile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "fiddle(\"convolution.cpp\", function=\"convolution_tiled_fixed_tile\", analyze=False, name=\"convolution_tiled_fixed_tile\", run=[\"perf_count\"], tagged_only=False, opt=\"-O3\",\n",
    "            cmdline=f\"--size {64*1024} --size2 {16*1024} --size3 {64*1024}  --tile-size 1024 \", perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n",
    "data = render_csv(\"convolution.csv\").append(render_csv(\"convolution_tiled_unrolled.csv\"))\n",
    "data = data.append(render_csv(\"convolution_tiled_split.csv\")).append(render_csv(\"tile_size.csv\").iloc[6]).append(render_csv(\"convolution_tiled_fixed_tile.csv\"))\n",
    "display(data[[\"function\", \"tile_size\",\"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "That's worth another 3% and brings up the total speedup (all from reduced `IC`) to 0.47/0.26 = 1.74x.\n",
    "\n",
    "Ok.  We are now done with 1-D convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Programming Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "For your programming assignment in this lab you'll be optimizing a specialized version of matrix multiply called \"matrix exponentiation\".  The input to your program will be a square matrix, $M$, (stored in a `tensor_t`) and a power, $p$, and your job is to compute $M^p$ as quickly as possible.\n",
    "\n",
    "The expression $M^p$ means $M$ multiplied by itself, $p$ times, where multiplication is normal matrix multiplication.\n",
    "\n",
    "This computation has a variety of applications.  For instance, you can use this algorithm to evaluate Markov Chains.\n",
    "\n",
    "For this assignment we'll be computing on `tensor_t<uint64_t>`.  Many applications would use `float` or `double`, but that problem is harder due [numerical instability](https://en.wikipedia.org/wiki/Numerical_stability) issues.  You don't need to worry about integer overflow in this assignment.  It will happen a lot, and it's consider the \"correct\" behavior for the purposes of this lab.\n",
    "\n",
    "## Reference Code\n",
    "\n",
    "The reference implementation is in `matexp_reference.hpp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"matexp_reference.hpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Read through the code and comments to make sure you understand what the code is doing. \n",
    "\n",
    "## Detailed Requirements\n",
    "\n",
    "The requirements for the lab are pretty simple:\n",
    "\n",
    "1. $M$ will be square and it's width/height will be less than 2048.\n",
    "2. $p$ will be less than or equal to 1024.\n",
    "3. $p$ will be greater than or equal to 0.\n",
    "4. Like `matexp_reference`, your function need to be a template function, but you can assume that `T` is always `uint64_t`.\n",
    "5. Values in $M$ can be any `uint64_t` value.\n",
    "6. Your output must match the output of the code in `matexp_reference.hpp`.\n",
    "7. Your implementation should go in `matexp_solution.hpp`.  The starter version is just a copy of `matexp_reference.hpp`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Running the Code\n",
    "\n",
    "The driver code for the lab is in `matexp_main.cpp` and `matexp.cpp`.  `matexp_main.cpp` is mostly command line processing (take a look if you want).  `matexp.cpp` is what actually calls your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"matexp.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "It defines four functions:\n",
    "\n",
    "* `matexp_reference_c` Calls the starter code with `size`x`size` matrix and `power`.\n",
    "* `matexp_solution_c` Calls your code with `size`x`size` matrix and `power`.\n",
    "* `bench_reference` Runs benchmarks we will use for grading for the starter code.\n",
    "* `bench_solution` Runs benchmarks we will use for grading for your code.\n",
    "\n",
    "It runs everything 8 times to get a more reliable measurement.\n",
    "\n",
    "To invoke these, you can build and run `matexp.exe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!./matexp.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "`matexp.exe` takes several command line parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!./matexp.exe --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The notable ones are:\n",
    "\n",
    "1. `--size` -- set the size of the matrix to multiply.\n",
    "2. `--power` -- set the power to raise it to.\n",
    "3. `--p1` to `--p5` -- set parameters (see below.)\n",
    "4. `--function` what functions to run.\n",
    "5. `--seed` set the random seed.\n",
    "6. `--stats-file` sets where statistics should go.\n",
    "\n",
    "The first five of these can take multiple values and `matexp.exe` will run all combinations and they will end up in `stats.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!./matexp.exe --function matexp_reference_c matexp_solution_c --size 10 20 --power 4 8\n",
    "render_csv(\"stats.csv\", columns=[\"function\", \"size\", \"power\", \"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "And, of course, we run it all in the cloud (we've added `--stat-set L1.cfg` to gather cache and TLB statistics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!cse142 job run --lab caches2 \"./matexp.exe --stat-set L1.cfg --function matexp_reference_c matexp_solution_c --size 10 20 --power 4 8\"\n",
    "render_csv(\"stats.csv\", columns=pa_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Setting Parameters\n",
    "\n",
    "One of the key parts of this lab is setting parameters (e.g., tiling sizes), and the `matexp.exe` has support for this built in vias the `--p1` -- `--p5` command line options and function parameters.\n",
    "\n",
    "You can use these for whatever you'd like:\n",
    "\n",
    "1.  Setting tile sizes.\n",
    "2.  Selecting among different implementations.\n",
    "3.  Whatever else.\n",
    "\n",
    "Their default value is 1.\n",
    "\n",
    "Just like `--size` and `--power`, you can multiple values and `matexp.exe` will run all combinations.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!cse142 job run --lab caches2 \"./matexp.exe --stat-set L1.cfg --function matexp_reference_c matexp_solution_c --size 10 20 --power 4 8 --p1 1 2 --p2 3 4 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_csv(\"stats.csv\", columns=pa_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Tips for Using Parameters**\n",
    "\n",
    "1.  Running multiple values of multiple parameters can result in a lot of experiments... sometimes too many.\n",
    "2.  Jobs in the cloud are limited to 5 minutes, so you need to limit the number of tests per job.\n",
    "3.  That said, exploring a wide space of parameter settings can be an effective way to optimize your code.  There are tips in the \"Looking at Data Section\" about how to deal with lots of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## The Test Suite\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Tests are great:** Tests are about the best thing ever (although writing them is a hassle). If you run the tests consistently, you can worry _much_ less about correctness.  Make small incremental changes to your code, run the tests after each change and enjoy the warm glow of happiness when they pass!\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**NOTE:** You normally will not need to run `run_tests.exe` in the cloud.  It'll work fine, but it takes longer which will slow your work down.  The test suite is about _correctness_ not performance.\n",
    "     \n",
    "</div>\n",
    "\n",
    "The lab  provides a comprehensive test suite for your implementation.  The code in is `run_test.cpp`.  `run_tests.exe` also takes the `--p*` arguments so you can run the regressions with different parameter settings.  This is a _good_ idea.\n",
    "\n",
    "You can build the tests with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make run_tests.exe\n",
    "!./run_tests.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Test Suite Details\n",
    "\n",
    "You can list all the tests with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe --gtest_list_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The first group of tests is (under `MatexpTests`), contains four simple tests that call `do_simple_diag_test()` and `do_simple_offdiag_test()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"run_tests.cpp\", show=(\"//START1\", \"//END1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "These two functions take a diagonal or off-diagonal matrix and raise them to a power.  It's easy to calculate the correct results for these computations, so they make good tests.\n",
    "\n",
    "These tests are:\n",
    "\n",
    "1. `one_test` runs the two functions above for a given size.\n",
    "2. `simple_tests` runs the two functions above for a set of small test cases.\n",
    "3. `simple_random_tests` runs the two functions for 10 randomly generate test cases.\n",
    "\n",
    "The final test in this group (`randomize_tests`) calls `do_test()` which compares the output `matexp_reference.hpp` with `matexp_solution.hpp` for randomized test cases.\n",
    "\n",
    "The second group of tests the `size`s and `power`s that the benchmark uses.\n",
    "\n",
    "The third group of tests (under `MatexpTests/MatexpTestFixture`) calls `do_test` with a bunch of test cases of various sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Things To Try\n",
    "\n",
    "There are two main challenges I see in this lab:\n",
    "\n",
    "1. Make matrix multiplication fast, primarily by improving it's memory behavior.\n",
    "2. Applying matrix multiplication efficiently to compute $M^p$.\n",
    "\n",
    "The benchmarks are structured to evaluate your solution's success on both of these challenges.\n",
    "\n",
    "### Tiling Matrix Multiplication\n",
    "\n",
    "#### For next time\n",
    "\n",
    "Tiling is worth about 1.2x.  Mention -- optimization for special cases. Efficient copying.\n",
    "\n",
    "\n",
    "The obvious approach to improving cache performance is tiling and renesting.  You saw an example of this with 1-D convolution, and the principle is the same, but the problem is a little more complex because there is an extra loop.\n",
    "\n",
    "There are two ways to approach this task and you should try to apply both at once:\n",
    "\n",
    "1.  You should think about the data access pattern of matrix multiply in terms of temporal and spatial locality.  \n",
    "    1.  How can you maximize spatial locality?\n",
    "    2.  Don't forget to consider all three matrices.\n",
    "    3.  How large can the tile size be while still fitting in the cache?\n",
    "2.  You should try different tiling schemes:\n",
    "    1.  Different ways to split and renest the three loops.\n",
    "    2.  Different tile sizes (`--p1` to `--p5` are provided for this purpose)\n",
    "3.  Debugging tiling\n",
    "    1.  Debugging tiling can be tricky.\n",
    "    2.  Start with small matrices and small tile sizes.\n",
    "    3.  Try multiple small tile sizes (pass `--p*` to the regressions)\n",
    "3.  Don't forget about loop overhead.\n",
    "\n",
    "There's a nice [Wikipedia page](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm) about matrix multiplication.  It covers the theory behind implementing it effectively.  The content is good, but don't assume that theory and practice will match.\n",
    "\n",
    "\n",
    "### Raising to a Power\n",
    "\n",
    "Computing $M^p$ can done more efficiently than multiplying $M$ by itself $p$ times (which is what the reference code does).  By way of a hint, remember that:\n",
    "\n",
    "$$M^{p+q} = M^pM^q$$\n",
    "\n",
    "As you work on this part of the problem, I suggest practicing with integers first.  I found it useful to code my solution with integers and test it and then rewrite it for matrices.\n",
    "\n",
    "### Using the Test Suite\n",
    "\n",
    "The test suite is meant to help keep you on the right track as you go through the assignment.  When you make a change to you code, I would:\n",
    "\n",
    "1.  Run all the tests.  If they all pass, great!\n",
    "2.  If some fail, run `simple_tests` and then `simple_random_tests`.\n",
    "3.  Once I find a particular test case that fails, I'd use `one_test` to run just that configuration while debugging.\n",
    "\n",
    "If and when you make use of parameters (`--p1` etc.), I'd try out the values of interest with `./run_tests.exe` before bothering to run code in the cloud.\n",
    "\n",
    "One debugging tip:  `tensor_t.hpp` includes support for the `<<` operator so you can say\n",
    "\n",
    "```\n",
    "std::cerr << my_tensor\n",
    "```\n",
    "\n",
    "This can be very helpful when debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Useful C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "There are few things in C++ that might be useful in this lab.\n",
    "\n",
    "### Controlling Compiler Optimizations\n",
    "\n",
    "First, you can prevent inlining of a particular function by declaring it like so:\n",
    "\n",
    "```\n",
    "void __attribute__((noinline)) matexp_solution(...)\n",
    "```\n",
    "\n",
    "This can make it easier to debug, because you can set a breakpoint on the function and it'll work like you expect.\n",
    "\n",
    "Second, you can turn on arbitrary optimizations for particular functions like so:\n",
    "\n",
    "```\n",
    "#pragma GCC push_options\n",
    "#pragma GCC optimize (\"unroll-loops\")\n",
    "\n",
    "void your_function() {\n",
    "}\n",
    "\n",
    "#pragma GCC pop_options\n",
    "```\n",
    "\n",
    "\n",
    "### Assertions\n",
    "\n",
    "The `assert()` macro is useful tool for debugging and to avoid silly errors.\n",
    "\n",
    "If you say\n",
    "\n",
    "```\n",
    "assert(a > b);\n",
    "```\n",
    "\n",
    "And the expression is not true at run time, the assert with \"fail\" your program will crash with a somewhat useful error message.\n",
    "\n",
    "This is a useful way to document and enforce assumptions you make in your code.  For instance, I used an assert in `convolution_tiled_split()` to ensure that the tile size was > 8.\n",
    "\n",
    "You can get access to  `assert()` with \n",
    "\n",
    "```\n",
    "#include<cassert>\n",
    "```\n",
    "\n",
    "The overhead of asserts is low, but not zero.  I would not put any in one of your performance-critical loops.\n",
    "\n",
    "If you want to include asserts in performance-critical areas, you can add `-DNDEBUG` to the optimizations in `config.make`.  It'll disable all the `assert()`s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Do Your Work Here\n",
    "\n",
    "Below are the key commands you'll need to make progress on the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Setting Optimization Flags\n",
    "\n",
    "As in your last lab, you can set optimization flags in `config.make`.  The baseline includes `-Og`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"config.make\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Compiling and Running\n",
    "\n",
    "You can compile and the benchmarks locally using this command.  This is only useful for debugging.  Performance running locally is not very meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!./matexp.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run the benchmark in the cloud and compare your performance with the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!cse142 job run --lab caches2 --take ./matexp.exe --take L1.cfg  --force \"./matexp.exe --MHz 3500 --stat-set ./L1.cfg  --function bench_solution  --p1 1 --p2 1 --p3 1  --p4 1 --p5 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "render_csv(\"stats.csv\", columns=pa_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "This command will approximate what the autograder will do, but it let's you pass `--p*` parameters.  The cells below will show your results and what the autograder will do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!cse142 job run --lab caches2 --force  \"./matexp.exe --MHz 3500 --stats bench.csv --stat-set ./L1.cfg --function bench_solution  --p1 1 --p2 1 --p3 1 --p4 1 --p5 1\"\n",
    "render_csv(\"bench.csv\", columns=pa_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "from autograde import compute_all_scores\n",
    "df = compute_all_scores(dir=\".\")\n",
    "display(df)\n",
    "print(f\"total points: {round(sum(df['capped_score']), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Running Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Build the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make run_tests.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Here's a good order run the tests in.\n",
    "\n",
    "**Run one test**\n",
    "\n",
    "This is most useful for debugging.  Running a single test doesn't tell you much about your code's correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe --gtest_filter=MatexpTests.one_test --size 10 --power 20 --p1 1 --p2 1 --p3 1 --p4 1 --p5 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Run the simple tests**\n",
    "\n",
    "Do this for checking basic correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe --gtest_filter=MatexpTests.simple* --p1 1 --p2 1 --p3 1 --p4 1 --p5 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Run more regressions (everything except the benchmark regressions with take a long time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe --gtest_filter=-MatexpBench*  --p1 1 --p2 1 --p3 1 --p4 1 --p5 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Run all the tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe   --p1 1 --p2 1 --p3 1 --p4 1 --p5 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Looking at Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The `--p*` command line options and parameters give you the ability to test many different configurations for your algorithms.  You should use them!  \n",
    "\n",
    "The result will be lots of data in lots of csv files.  For instance, if you run the command in the previous section, you'll get `stats.csv`.  Let's generate another stats file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!cse142 job run --lab caches2 --force \"./matexp.exe --MHz 3500 --stats other_stats.csv --stat-set ./L1.cfg  --function bench_solution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "  You can load and view several at once like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = render_csv([\"stats.csv\", \"other_stats.csv\"], columns=pa_columns).sort_values(by=\"ET\").head(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "That command collects all the `.csv` files provide, selects a useful set of columns for this PA, sorts by `ET`, and display the top 2 elements.\n",
    "\n",
    "If you'd rather work on the data in Excel (or whatever), you can export it as a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"my_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Tools\n",
    "\n",
    "These are some tools you might find useful as you optimize your implementation.  I encourage you to give some of them a try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Debugging Regressions \n",
    "\n",
    "If a regression fails, `run_tests.exe` will tell you which test failed.  Here are some tips for debugging.  First, get a list of the tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "One of them will be the test that failed.  Then you can debug in gdb (at a terminal again):\n",
    "\n",
    "```\n",
    "bash$ gdb run_tests.exe\n",
    "(gdb) run --gtest_filter=<name_of_failing_test> --gtest_break_on_failure\n",
    "```\n",
    "The `--gtest_filter` just runs one test.  and `--gtest_break_on_failure` will stop drop you into the debugger if the error occurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Looking At Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "As you learned in the previous lab, name mangling makes it a little tricky to inspect the details of what the compiler does to C++ code, especially when it uses templates.  So let's see how we can track down the assembly for for your implementation.\n",
    "\n",
    "The `Makefile` is set up to build assembly files (ending in `.s`) in the `build` directory.  All the assembly for `matexp_solution.hpp` and `matexp_reference.hpp` (and a whole bunch of other stuff) will be in `matexp.s`.   It's quite long, so searching through it by hand is daunting.  To make matters worse, all the function names are mangled.\n",
    "\n",
    "One solution to this is to `c++filt` to demangle the names and the use `grep` to find the symbols of interest (the `^` matches the beginning of the line`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make build/matexp.s\n",
    "!c++filt < build/matexp.s | grep '^void matexp_solution<unsigned long>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You may see multiple functions listed with suffixes like `[clone .constprop.163]`.  These are specialized versions of the function that gcc produced.  They have had constant propagation applied to them for particular sets of values.  Initially, at least, I'd pay most attention to the generic version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You can see that there are several different versions of each method, one for each set of template parameters.  Unless you're doing something very sophisticated with your implementations, the assembly will all be basically the same.\n",
    "\n",
    "You can now render the assembly here with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make build/matexp.s\n",
    "render_code(\"build/matexp.s\", show=\"void matexp_solution<unsigned long>(tensor_t<unsigned long>&, tensor_t<unsigned long> const&, unsigned int, long, long, long, long, long)\", lang=\"gas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Looking at the CFG\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**CFG Viewer Problems:**  In testing the lab, I've found that the CFG viewer is having trouble displaying some of the functions.  The symptom is that it runs forever.  If that happens, take a look at the assembly instead.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "Control flow graphs are easier to interpret than the assembly, but getting them for C++ functions is also a little complicated.  The tool that our CFG generator is built on uses its own name mangling scheme internally.  To get the names it uses for your functions you can use the command below.  We pass the executable to `cfg` along with `--filter` which takes a string to search for.  If you leave out `--filter` you will all 2890 symbols in the executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make matexp.exe\n",
    "!cfg matexp.exe --filter mult_reference --list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "There's a one-to-one correspondence between these names the names we saw earlier.  You can render a CFG like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "do_cfg(\"matexp.exe\", symbol=\"sym.void_mult_reference_unsigned_long__tensor_t_unsigned_long___tensor_t_unsigned_long__const__tensor_t_unsigned_long__const_\" , output=\"matexp4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "It's probably a bit of a mess.  I suggest copying it, downloading it, or opening it in it's own tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Profiling \n",
    "\n",
    "Profiling can be valuable tool in figuring out where your code is spending time.  \n",
    "\n",
    "To profile your allocator, you need to recompile it with profiling enabled:\n",
    "\n",
    "**NOTE:** Don't forget to rebuild without `GPROF=yes`. BUilding in support for gprof will slow down your code a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make clean matexp.exe GPROF=yes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You'll need to profile one type of benchmark at a time.  just run one of the lines below at a time.  For good accuracy, you should profile in the cloud.  \n",
    "\n",
    "Think carefully about to profile:\n",
    "\n",
    "1.  It's not a bad idea to profile the whole benchark, but it can be a bit hard to interpret the results, because there's a lot going on.\n",
    "2.  The results are sometimes clearer if you focus on just one test case.\n",
    "3.  Make sure you run a large enough test so that `matexp.exe` spends the vast majority of its time in your code.  This can be surprisingly large: `--size 600` is a good place start.\n",
    "\n",
    "Here's how to profile in the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!cse142 job run --lab caches2 --take matexp.exe --force \"./matexp.exe --MHz 3500 --function bench_reference; gprof ./matexp.exe > gprof.out\"\n",
    "!cat gprof.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The output is a big for Jupyter Notebook.  In a terminal you can do:\n",
    "\n",
    "```\n",
    "less -S gprof.out\n",
    "\n",
    "```\n",
    "\n",
    "Which will let you look at the file without wrapped lines.\n",
    "\n",
    "You may notice some unfamiliar functions.  Here's what some of them are:\n",
    "\n",
    "1.  `nlohmann::basic*`: This is a json library.  If you spend much time here, you aren't running a large enough test case.\n",
    "2.  `boost::*` or `OptionSpec` various utilities for command line parsing and parameter passing.  Again, if you see this, run a larger test case.\n",
    "3.  `mcount` is the gprof instrumentation function.  It gets called a lot, but it's not there.\n",
    "\n",
    "Another problem you may run into:  the compiler may inline everything so all the time is one function.  This is not very informative.  You can get around by turning off inlining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make clean matexp.exe GPROF=yes OPTIMIZE=\"-O3\"\n",
    "!cse142 job run --lab caches2 --force \"./matexp.exe --MHz 3500 --function matexp_solution_c --size 350 --power 10; gprof -l ./matexp.exe > gprof.out\" # Run one test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!cat gprof.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You might notice that performance drops significantly!  The resulting profile has more detail, but it's also not as accurate a reflection of your real program.  However, it can provide useful guidance about where you code is spending time.  YOu have to be careful though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Debugging \n",
    "\n",
    "Your code will certainly have errors in it, and you'll need to debug.  THe first thing you need to do is to tone down the optimizations, because they make debugging almost impossible.  Recall that `-Og` is the right flag to use for optimization while debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make clean matexp.exe OPTIMIZE=\"-Og\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Unfortunately, the Linux debugger `gdb` doesn't work inside the note book.  If you want to use it, you can do so at the terminal:\n",
    "\n",
    "```\n",
    "$ gdb matexp.exe\n",
    "GNU gdb (Ubuntu 8.1.1-0ubuntu1) 8.1.1\n",
    "Copyright (C) 2018 Free Software Foundation, Inc.\n",
    "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n",
    "This is free software: you are free to change and redistribute it.\n",
    "There is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\n",
    "and \"show warranty\" for details.\n",
    "This GDB was configured as \"x86_64-linux-gnu\".\n",
    "Type \"show configuration\" for configuration details.\n",
    "For bug reporting instructions, please see:\n",
    "<http://www.gnu.org/software/gdb/bugs/>.\n",
    "Find the GDB manual and other documentation resources online at:\n",
    "<http://www.gnu.org/software/gdb/documentation/>.\n",
    "For help, type \"help\".\n",
    "Type \"apropos word\" to search for commands related to \"word\"...\n",
    "Reading symbols from matexp.exe...rdone.\n",
    "(gdb) run --function matexp_solution_c --size 30 --power 3\n",
    "Starting program: /cse142L/labs/CSE141pp-Lab-Caches-II/matexp.exe --function matexp_solution_c --size 30 --power 3\n",
    "warning: Error disabling address space randomization: Operation not permitted\n",
    "[Thread debugging using libthread_db enabled]\n",
    "Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n",
    "registering function: bench_solution\n",
    "registering function: bench_reference\n",
    "registering function: matexp_reference_c\n",
    "registering function: matexp_solution_c\n",
    "Loading Native engine.\n",
    "Gonna run matexp_solution_c\n",
    "Running matexp_solution_c\n",
    ".\n",
    "[Inferior 1 (process 61156) exited normally]\n",
    "(gdb)\n",
    "```\n",
    "\n",
    "The best place to start is at `matexp_solution_c`.  From there you can step into your solution code.\n",
    "\n",
    "```\n",
    "bash$ gdb alloc_main.exe\n",
    "(gdb) break matexp_solution_c\n",
    "(gdb) run --function matexp_solution_c --size 100  --power 2\n",
    "(gdb) list\n",
    "```\n",
    "\n",
    "Sometimes that will note give a good result, even without optimizations.  Instead, you can set a break point at a line number:\n",
    "\n",
    "```\n",
    "bash$ gdb alloc_main.exe\n",
    "(gdb) break matexp_solution.hpp:47\n",
    "(gdb) run --function matexp_solution_c --size 100  --power 2\n",
    "(gdb) list\n",
    "```\n",
    "\n",
    "There's a pretty good `gdb` [tutorial here](https://www.cs.cmu.edu/~gilpin/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Tracing With Moneta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Moneta was originally built specifically to help with visualizing cache tiling optimizations.  Here's a few tips for getting good results:\n",
    "\n",
    "1.  Datahub can only handle traces with about 10 million memory operations, so keep the test cases small. `--size 60` is good.\n",
    "2.  Use the code in `matexp_reference.hpp` as a guide for how to tag the matrices.\n",
    "3.  Click on \"Accesses layers\" in the trace viewer to see which tags are which.\n",
    "4.  Click on \"None\" next to the gold square and select \"misses-all\" to see where the misses are.\n",
    "5.  Click on \"tags\" and then click on the check boxes to show and hide tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make clean matexp.exe \n",
    "!mtrace --memops 10000000 -- ./matexp.exe --size 60 --power 2 --function matexp_reference_c --p1 1 --p2 1 --p3 1 --p5 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "show_trace(\"./trace_0\",  show_tag=[\"dst\", \"B\", \"A\"], layer_preset=[\"None\", \"dst\", \"B\", \"A\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Final Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "When you are done, make sure your best allocator is called `matexp_solution()` in `matexp_solution.hpp`.  Your code will be invoked with `--p1` to `--p5` equal to 1, so you'll need to \"bake in\" the optimal values for those parameters.\n",
    "\n",
    "Then you can submit your code to the Gradescope autograder.  It will run the commands given above and use the `ET` values from `autograde.csv` to assign your grade.\n",
    "\n",
    "Your grade is based on your speed up relative `matexp_reference.hpp` on three benchmarks. \n",
    "\n",
    "For each of them, there's a target speedup given in the table.  You get a score for each benchmark between 0 and 33.3, and the overall score is the sum of these scores.  For each function, the score is compute as `your_speedup/target_speedup * 33.3`.\n",
    "\n",
    "For this lab, you don't get extra credit for beating the targets.  This will help ensure that your design in balanced:  You much do well at all 3 benchmarks to do well on the lab.\n",
    "\n",
    "To get points, your code must also be correct.  The autograder will run the regressions in `run_tests.exe` to check it's correctness.\n",
    "\n",
    "You can mimic exactly what the autograder will do with the command below.  You can run the cell below to list them and the target speedups.\n",
    "\n",
    "After you run it, the results will be in `autograde/autograde.csv` rather than `./bench.csv`.  This command builds and runs your code in a more controlled way by doing the following:\n",
    "\n",
    "1.  Ignores all the files in your repo except `matexp_solution.hpp` and `config.make`.\n",
    "2.  Copies those files into a clean clone of the starter repo.\n",
    "3.  Builds `matexp.exe` from scratch.\n",
    "4.  And then runs the commands the benchmarks.\n",
    "5.  It then runs the `autograde.py` script to compute your grade.\n",
    "\n",
    "Running the cell does just what the Gradescope autograder does.  And the cell below shows the name and target speedups for each benchmark.  This takes 1-2 minutes to run.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Only Gradescope Counts** The scores produced here **do not** count.  Only gradescope counts.  The results here should match what Gradescope does, but I would test your solution on Gradescope well-ahead of the deadline to ensure your code is working like you expect.\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**The autograder doesn't pass parameters**  The autograder will not pass any `-p*` parameters to your code.   You'll need to set it up so the default value (1 for all the `--p*` arguments) configures your code in the best way possible.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make matexp.exe\n",
    "!cse142 job run --take matexp_solution.hpp --take config.make --lab caches2-bench --force  autograde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "And run the autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./autograde.py --submission autograde --results autograde.json\n",
    "from autograde import compute_all_scores\n",
    "df = compute_all_scores(dir=\"autograde\")\n",
    "display(df)\n",
    "print(f\"total points: {round(sum(df['capped_score']), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The \"capped_score\" column contains the number of points you'll receive.\n",
    "\n",
    "And see the autograder's output like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_code(\"autograde.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Most of it is internal stuff that gradscope needs, but the key parts are the `score`, `max_score`, and `output` fields.\n",
    "\n",
    "All that's left is commit your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!git commit -am \"Solution to the lab.\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "If `git push` asks for your username, you'll need to push from the command line.\n",
    "\n",
    "If `git commit` tells you have uncommitted files, that's not a problem. \n",
    "\n",
    "If `git commit` tell you something like:\n",
    "\n",
    "```\n",
    "*** Please tell me who you are.\n",
    "\n",
    "Run\n",
    "\n",
    "git config --global user.email \"you@example.com\"\n",
    "git config --global user.name \"Your Name\"\n",
    "\n",
    "to set your account's default identity.\n",
    "Omit --global to set the identity only in this repository.\n",
    "\n",
    "fatal: unable to auto-detect email address (got 'prcheng@dsmlp-jupyter-prcheng.(none)')\n",
    "Warning: Permanently added the RSA host key for IP address '140.82.112.3' to the list of known hosts.\n",
    "Everything up-to-date\n",
    "```\n",
    "\n",
    "Then you can do (but fill in your @ucsd.edu email and your name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cse142L.is_response": true,
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!git config --global user.email \"you@example.com\"\n",
    "!git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "This lab completes our tour of (single processor) memory systems.  It explored what's required to exploit temporal locality and when it does and does not exist.  It also looked at other key components of the memory hierarchy:  The lower-level caches and the TLB.   Finally, it developed an optimized version of 1-D convolution using tiling and renesting, and you got to apply those concepts to computing $M^p$.  You should now be well-prepared for the next lab, where we will explore (among other things) how multiple processors further-complicate the performance of the memory hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Turning In the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "For each lab, there are two different assignments on gradescope:\n",
    "\n",
    "1.  The lab notebook.\n",
    "2.  The programming assignment.\n",
    "\n",
    "There's also a pre-lab reading quiz on Canvas and a post-lab survey which is embedded below.\n",
    "\n",
    "## If You Have Trouble\n",
    "\n",
    "If it's near the deadline and you are having trouble turning in any part of your lab, you can fill out this form: https://forms.gle/ThHjESfbZRqqztXUA to let us know what's going on and provide us access to the work you have done prior to the deadline.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**NOTE:** Filling out the form above _before_ the deadline is the _only_ mechanism available to receive credit without turning in the assignment on time.\n",
    "    \n",
    "</div>\n",
    "\n",
    "If it's more than a day before the deadline, you can reach out via Piazza and hopefully we can get it sorted out.\n",
    "\n",
    "\n",
    "## Reading Quiz\n",
    "\n",
    "The reading quiz is an online assignment on Canvas.  It's due before the class when we will assign the lab.\n",
    "\n",
    "## The Note Book\n",
    "\n",
    "You need to turn in your lab notebook and your programming assignment separately. \n",
    "\n",
    "After you complete the lab, you will turn it in by creating a version of the notebook that only contains your answers and then printing that to a pdf.\n",
    "\n",
    "**Step 1:**  Save your workbook!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!for i in 1 2 3 4 5; do echo Save your notebook!; sleep 1; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Step 2:**  Run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!turnin-lab Lab.ipynb\n",
    "!ls -lh Lab.turnin.ipynb"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAACUAAAAgCAYAAACVU7GwAAABQ2lDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rACYQiDNwMeonJxQWOAQE+QCUMMBoVfLvGwAiiL+uCzBKZ0nzEVv7xlbCa77O3fegRxFSPArhSUouTgfQfIE5LLigqYWBgTAGylctLCkDsDiBbpAjoKCB7DoidDmFvALGTIOwjYDUhQc5A9g0gWyA5IxFoBuMLIFsnCUk8HYkNtRcEeFxcfXwUAkyMDc0DCTiXdFCSWlECop3zCyqLMtMzShQcgaGUquCZl6yno2BkYGTIwAAKc4jqn2/BYclYtxkhlhjMwGDYChQUQohlizIw7PnNwCC0GyGmlcfAINjAwLA/viCxKBHuAMZvLMVpxkYQtngYAwNn1///L4Ae5f4HtEvv//8fvP///65nYGAvYmDotgIAsv9dz+zV67cAAAA4ZVhJZk1NACoAAAAIAAGHaQAEAAAAAQAAABoAAAAAAAKgAgAEAAAAAQAAACWgAwAEAAAAAQAAACAAAAAAe+CsRQAAAshJREFUWAntl89LKlEUx786kotQaaMh2KYnprixQlBeq/ZBgUtduvI/E1y0aRkuJDcp1ErQVSsNlELSovE9v9d3ffOjGWcg0UUHhnPv3PPj4znXe9XzZy7YMvFuGY/A2Uoon7ZS3V4P7VYL7+/v2tdrHfv9fmSOj/Hr8HCZRwfVur9HNBqFoijCwOPxLA2/eyC3sqqqohCWUB8fH/D5fCCM17vo7DrAJNBsNhO5jJ3RVYrVYJUIRBj5rKNKBGMeghnFEsrrJdz3gxGGz2zGRxUffCUU6dlCam3FpONoNEK/30csFsPr6yteXl7kkqWORCLY29sT6/+hZnMoDz4/P01+pkotKuT910ZlrvWnxs3NDWq1GiqVCur1Oh4fH01BjS+urq5QKpWWr1WVe0kVc+YzigmKEIsqKfOKKcvyyg0fj8eRz+dxcHCATCaDQCBgjGma04d7lVWiMBYLxLmimC8UE5R0Ipzc6BKIa+FwGIlEAqFQSLRQHh9csxL6yFgSjPHZvq/EBCWdpbFx3mw2Ua1WEQwGcXt7i4eHB2lqqQuFAo6OjpbrEswYWxqYoOQCNZ2MksvlRMtSqZTQp6enRhPTPJ1O6959FVdrYAulNZTjweAZnU4HyWQST09P6Ha7cslSh8MR0XJLA8OCa6her4tGowFWqN1uO2ofr66zs9+G1NZT11AXFxfIZrPi28cW8qxaJfv7+6tMdOuuoXg2XV9fo1wu4+7uztE5xQ9yeXmpS2w3cQ01Ho8xGAzw9vaG4XAoxnYJuEYfN+Iaqlgsgg/l/PzcTS7Htvo7xLHbeg1/oJzW17ZS8jpwGsyp3aq4po1OB62Tduw0qZ2dNp4xl/TTQe3s7GA6nWJ3dxeL3zyLu4/Oq+4rGdBOSyBqxqeeTCbgPxqt6KCOT0429hdLC+WZ05p/ZWktNjC23egb4BEpf6CcVn4rK/UXyU7+WMGqw1YAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The date in the above file listing should show that you just created `Lab.turnin.ipynb`\n",
    "\n",
    "**Step 3:**  Click on this link to open it: [./Lab.turnin.ipynb](./Lab.turnin.ipynb)\n",
    "\n",
    "**Step 4:**  Hide the table of contents by clicking the\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "**Step 5:**  Select \"Print\" from _your browser's_ \"file\" menu.  Print directly to a PDF.\n",
    "\n",
    "**Step 6:**  Make sure all your answers are visible and not cut off the side of the page.\n",
    "\n",
    "**Step 7:**  Turn in that PDF via gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Print Carefully** It's important that you print directly to a PDF.  In particular, you should _not_ do any of the following:\n",
    "    \n",
    "1. **Do not** select \"Print Preview\" and then print that. (Remarkably, this is not the same as printing directly, so it's not clear what it is a preview of)\n",
    "2. **Do not** select `Download as-> PDF via LaTex.  It generates nothing useful.\n",
    "    \n",
    "</div>\n",
    "\n",
    "In gradescope, you'll need to show us where all your answers are.  Please do this carefully, if we can't find your answer, we can't grade it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## The Programming Assignment\n",
    "\n",
    "You'll turn in your programming assignment by providing gradescope with your github repo.   It'll run the autograder and return the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Lab Survey\n",
    "\n",
    "Please fill out this survey when you've finished the lab.  You can only submit once.  Be sure to press \"submit\", your answers won't be saved in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame    \n",
    "IFrame('https://docs.google.com/forms/d/e/1FAIpQLScHbK7yLlixJqdYsRnpvLLT_Ra8vdhmx8bE0KS50HYuiV4f4Q/viewform?embedded=true', width=800, height=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Ignore this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# My Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "from notebook import *\n",
    "def plot_consistency(file_list, bins=10, groupby=\"label\", FOM=\"ET\"):\n",
    "    \"\"\"\n",
    "    Take a list of CSV files, group them by a column, and measure te consistency of FOM.\n",
    "    \n",
    "    Use with run_consistency.sh and run_canary.sh check the reproducibility of results. \n",
    "    \"\"\"\n",
    "\n",
    "    df = render_csv(file_list)\n",
    "    g = df.groupby(by=groupby)\n",
    "    g.hist(column=FOM, bins=bins)\n",
    "    mean = g.mean(numeric_only=True)[FOM]\n",
    "    max = g.max(numeric_only=True)[FOM]\n",
    "    min = g.min(numeric_only=True)[FOM]\n",
    "    std = g.std()[FOM]\n",
    "    print(f\"N = {len(df)/len(std)}\")\n",
    "    print(\"MEAN:\")\n",
    "    display(mean)\n",
    "    print(\"STD:\")\n",
    "    display(std)\n",
    "    error = std/mean\n",
    "    print(\"STD/mean:\")\n",
    "    display(error)\n",
    "    print(\"max-min/mean:\")\n",
    "    display((max - min)/(mean))\n",
    "    return g.mean(numeric_only=True).copy()\n",
    "\n",
    "plot_consistency(\"consistency/scores-*.csv\", bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!cse142 job run --lab caches2 --take ./matexp.exe --take L1.cfg  --force \"./matexp.exe --MHz 3500 --stat-set ./L1.cfg --stats 120-big.csv --function matexp_solution_c --size 120 --power 2 --p1 1 5 10 15 20 25 30 35 40 45 55 60 65 70 75 80 85 90 95 100 200 300 400 500 600 --p2 1 --p3 1  --p4 1 --p5 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df=render_csv([\"stats.csv\", \"big.csv\"])\n",
    "display(df)\n",
    "plotPE(df=df, lines=True, what=[(\"p1\", \"L1_cache_misses\"), (\"p1\", \"ET\"), (\"p1\", \"L1_MPI\"), (\"p1\", \"IC\"), (\"p1\", \"CT\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df=render_csv(\"120-big.csv\")\n",
    "display(df)\n",
    "plotPE(df=df, lines=True, what=[(\"p1\", \"L1_cache_misses\"), (\"p1\", \"ET\"), (\"p1\", \"L1_MPI\"), (\"p1\", \"IC\"), (\"p1\", \"CT\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df=render_csv(\"320-big.csv\")\n",
    "display(df)\n",
    "plotPE(df=df, lines=True, what=[(\"p1\", \"L1_cache_misses\"), (\"p1\", \"ET\"), (\"p1\", \"L1_MPI\"), (\"p1\", \"IC\"), (\"p1\", \"CT\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "plotPE(df=df, lines=True, what=[(\"p1\", \"L1_cache_misses\"), (\"p1\", \"ET\"), (\"p1\", \"L1_MPI\"), (\"p1\", \"IC\"), (\"p1\", \"CT\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!cse142 job run --lab caches2 --take ./matexp.exe --take L1.cfg  --force \"./matexp.exe --MHz 3500 --stat-set ./L1.cfg --stats i-on-inside.csv --function bench_solution --p1 1 --p2 1 --p3 1  --p4 0 1 2 --p5 0 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data = render_csv(\"stats.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "t = [(3.19,2.2),(7.52,3.5),(2.9,18.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "base, speedups = zip(*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data[\"target_speedups\"] = speedups *6\n",
    "data[\"base\"] = base*6\n",
    "data[\"speedup\"] = data[\"base\"]/data[\"ET\"]\n",
    "data[\"score\"] = data[\"speedup\"]/data[\"target_speedups\"] *33\n",
    "data[\"score\"] = data[\"score\"].apply(lambda x: round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data[\"exp\"] = data[\"p4\"].apply(lambda x: [\"baseline\", \"opt\", \"unopt\"][x])\n",
    "data[\"mult\"] = data[\"p5\"].apply(lambda x: [\"linear\", \"binary\"][x])\n",
    "\n",
    "data[[\"p4\", \"p5\", \"exp\", \"mult\",\"ET\", \"size\", \"power\",\"base\", \"target_speedups\", \"speedup\", \"score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "t=data.groupby(by=[\"exp\", \"mult\"]).sum().copy()\n",
    "t[\"exp\"] = list(zip(*t.index))[1]\n",
    "t[\"opt\"] = list(zip(*t.index))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "t.index=range(0,len(t))\n",
    "t[[\"exp\", \"opt\", \"score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "t.index=range(0,len(t))\n",
    "t[[\"exp\", \"opt\", \"score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
